{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸš€ English-Vietnamese Translation Model\n",
                "## Train on Google Colab\n",
                "\n",
                "**Requirements:**\n",
                "- Runtime â†’ Change runtime type â†’ **GPU** (T4 recommended)\n",
                "- ~13 hours for 10 epochs with full data\n",
                "\n",
                "**Features:**\n",
                "- âœ… Auto-save checkpoint má»—i 15 phÃºt\n",
                "- âœ… Tá»± Ä‘á»™ng resume tá»« checkpoint náº¿u bá»‹ ngáº¯t\n",
                "- âœ… LÆ°u vÃ o Google Drive Ä‘á»ƒ khÃ´ng máº¥t dá»¯ liá»‡u\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Check GPU\n",
                "import torch\n",
                "print(f\"PyTorch: {torch.__version__}\")\n",
                "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
                "else:\n",
                "    print(\"âš ï¸ No GPU! Go to Runtime â†’ Change runtime type â†’ GPU\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Mount Google Drive (Ä‘á»ƒ lÆ°u checkpoint tá»± Ä‘á»™ng)\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "# Táº¡o thÆ° má»¥c lÆ°u checkpoint\n",
                "import os\n",
                "CHECKPOINT_DIR = '/content/drive/MyDrive/EV_Translation_Checkpoint'\n",
                "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
                "print(f\"âœ“ Checkpoint directory: {CHECKPOINT_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Clone repository\n",
                "!rm -rf EV_Translate_Modle_NLP_Project\n",
                "!git clone https://github.com/TranKien2005/EV_Translate_Modle_NLP_Project.git\n",
                "%cd EV_Translate_Modle_NLP_Project"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Install dependencies (skip torch - already installed on Colab)\n",
                "!pip install -q datasets sentencepiece sacrebleu google-generativeai python-dotenv tqdm tensorboard seaborn pyyaml huggingface_hub"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. Create .env with your tokens\n",
                "# REPLACE THESE WITH YOUR ACTUAL TOKENS!\n",
                "HF_TOKEN = \"YOUR_HUGGINGFACE_TOKEN\"  # Get from https://huggingface.co/settings/tokens\n",
                "GEMINI_API_KEY = \"YOUR_GEMINI_API_KEY\"  # Optional, for evaluation\n",
                "\n",
                "with open('.env', 'w') as f:\n",
                "    f.write(f'HF_TOKEN={HF_TOKEN}\\n')\n",
                "    f.write(f'GEMINI_API_KEY={GEMINI_API_KEY}\\n')\n",
                "\n",
                "print('âœ“ .env created')\n",
                "print('âš ï¸ Make sure to replace the placeholder tokens above!')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6. Download PhoMT dataset (~500MB)\n",
                "!python scripts/download_phomt.py"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 7. Preprocess data (may take 10-15 minutes for full dataset)\n",
                "# Use --max-samples 100000 for quick test\n",
                "!python scripts/preprocess_data.py"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 8. Configure for Colab training\n",
                "import yaml\n",
                "\n",
                "with open('config/config.yaml', 'r') as f:\n",
                "    cfg = yaml.safe_load(f)\n",
                "\n",
                "# Training settings\n",
                "cfg['data']['source'] = 'processed'\n",
                "cfg['training']['epochs'] = 10\n",
                "cfg['training']['batch_size'] = 128  # T4 has 16GB, can handle larger batch\n",
                "cfg['hardware']['num_workers'] = 2  # Colab works better with fewer workers\n",
                "\n",
                "with open('config/config.yaml', 'w') as f:\n",
                "    yaml.dump(cfg, f, default_flow_style=False)\n",
                "\n",
                "print('âœ“ Config updated for Colab')\n",
                "print(f\"  Epochs: {cfg['training']['epochs']}\")\n",
                "print(f\"  Batch size: {cfg['training']['batch_size']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ”„ Resume Training Setup\n",
                "\n",
                "Há»‡ thá»‘ng sáº½:\n",
                "1. Kiá»ƒm tra checkpoint trong Google Drive\n",
                "2. Náº¿u cÃ³ â†’ resume training tá»« checkpoint Ä‘Ã³\n",
                "3. Náº¿u khÃ´ng â†’ train tá»« Ä‘áº§u\n",
                "4. **Auto-save má»—i 15 phÃºt** vÃ o Google Drive"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 9. Kiá»ƒm tra checkpoint hiá»‡n cÃ³ trong Drive\n",
                "import os\n",
                "import shutil\n",
                "\n",
                "CHECKPOINT_FILE = os.path.join(CHECKPOINT_DIR, 'current_checkpoint.pt')\n",
                "TOKENIZER_DIR = os.path.join(CHECKPOINT_DIR, 'tokenizers')\n",
                "\n",
                "# Kiá»ƒm tra cÃ³ checkpoint khÃ´ng\n",
                "RESUME_FROM = None\n",
                "if os.path.exists(CHECKPOINT_FILE):\n",
                "    print(f\"âœ… Found existing checkpoint: {CHECKPOINT_FILE}\")\n",
                "    print(f\"   Size: {os.path.getsize(CHECKPOINT_FILE) / 1024 / 1024:.1f} MB\")\n",
                "    \n",
                "    # Copy checkpoint vá» local\n",
                "    shutil.copy(CHECKPOINT_FILE, 'checkpoints/resume_checkpoint.pt')\n",
                "    RESUME_FROM = 'checkpoints/resume_checkpoint.pt'\n",
                "    print(f\"   â†’ Copied to local for resume\")\n",
                "    \n",
                "    # Copy tokenizers náº¿u cÃ³\n",
                "    if os.path.exists(TOKENIZER_DIR):\n",
                "        os.makedirs('checkpoints/tokenizers', exist_ok=True)\n",
                "        for f in os.listdir(TOKENIZER_DIR):\n",
                "            shutil.copy(os.path.join(TOKENIZER_DIR, f), f'checkpoints/tokenizers/{f}')\n",
                "        print(f\"   â†’ Copied tokenizers from Drive\")\n",
                "else:\n",
                "    print(\"ðŸ“Œ No checkpoint found in Drive. Will train from scratch.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 10. Custom Trainer vá»›i Auto-Save má»—i 15 phÃºt\n",
                "import sys\n",
                "sys.path.insert(0, '.')\n",
                "\n",
                "import time\n",
                "import shutil\n",
                "import threading\n",
                "from src.train import Trainer\n",
                "from src.utils import save_checkpoint\n",
                "\n",
                "class AutoSaveTrainer(Trainer):\n",
                "    \"\"\"Trainer vá»›i tÃ­nh nÄƒng auto-save má»—i 15 phÃºt.\"\"\"\n",
                "    \n",
                "    def __init__(self, save_dir, save_interval=900, **kwargs):\n",
                "        \"\"\"\n",
                "        Args:\n",
                "            save_dir: ThÆ° má»¥c lÆ°u checkpoint (Google Drive)\n",
                "            save_interval: Khoáº£ng thá»i gian giá»¯a cÃ¡c láº§n lÆ°u (giÃ¢y), máº·c Ä‘á»‹nh 900s = 15 phÃºt\n",
                "        \"\"\"\n",
                "        super().__init__(**kwargs)\n",
                "        self.save_dir = save_dir\n",
                "        self.save_interval = save_interval\n",
                "        self.last_save_time = time.time()\n",
                "        self._stop_auto_save = False\n",
                "        \n",
                "    def _auto_save_checkpoint(self):\n",
                "        \"\"\"LÆ°u checkpoint hiá»‡n táº¡i vÃ o Drive.\"\"\"\n",
                "        if self.model is None:\n",
                "            return\n",
                "            \n",
                "        try:\n",
                "            # LÆ°u vÃ o local trÆ°á»›c\n",
                "            local_path = 'checkpoints/auto_save_temp.pt'\n",
                "            save_checkpoint(\n",
                "                self.model,\n",
                "                self.optimizer,\n",
                "                self.current_epoch,\n",
                "                self.best_val_loss if hasattr(self, 'best_val_loss') else float('inf'),\n",
                "                local_path,\n",
                "                best_val_loss=self.best_val_loss if hasattr(self, 'best_val_loss') else float('inf')\n",
                "            )\n",
                "            \n",
                "            # Copy sang Drive (ghi Ä‘Ã¨)\n",
                "            drive_path = os.path.join(self.save_dir, 'current_checkpoint.pt')\n",
                "            shutil.copy(local_path, drive_path)\n",
                "            \n",
                "            # LÆ°u tokenizers\n",
                "            tokenizer_dir = os.path.join(self.save_dir, 'tokenizers')\n",
                "            os.makedirs(tokenizer_dir, exist_ok=True)\n",
                "            if os.path.exists('checkpoints/tokenizers'):\n",
                "                for f in os.listdir('checkpoints/tokenizers'):\n",
                "                    shutil.copy(f'checkpoints/tokenizers/{f}', os.path.join(tokenizer_dir, f))\n",
                "            \n",
                "            self.last_save_time = time.time()\n",
                "            print(f\"\\nðŸ’¾ Auto-saved checkpoint to Drive at epoch {self.current_epoch}\")\n",
                "            \n",
                "        except Exception as e:\n",
                "            print(f\"\\nâš ï¸ Auto-save failed: {e}\")\n",
                "    \n",
                "    def train_epoch(self):\n",
                "        \"\"\"Override Ä‘á»ƒ thÃªm logic auto-save.\"\"\"\n",
                "        result = super().train_epoch()\n",
                "        \n",
                "        # Kiá»ƒm tra cÃ³ cáº§n auto-save khÃ´ng\n",
                "        elapsed = time.time() - self.last_save_time\n",
                "        if elapsed >= self.save_interval:\n",
                "            self._auto_save_checkpoint()\n",
                "        \n",
                "        return result\n",
                "\n",
                "print('âœ“ AutoSaveTrainer defined')\n",
                "print(f'  Auto-save interval: {900}s (15 minutes)')\n",
                "print(f'  Save directory: {CHECKPOINT_DIR}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 11. Start Training vá»›i Auto-Save!\n",
                "# Checkpoint sáº½ Ä‘Æ°á»£c lÆ°u vÃ o Google Drive má»—i 15 phÃºt\n",
                "# Náº¿u bá»‹ ngáº¯t, cháº¡y láº¡i tá»« cell 1 vÃ  nÃ³ sáº½ tá»± resume\n",
                "\n",
                "trainer = AutoSaveTrainer(\n",
                "    save_dir=CHECKPOINT_DIR,\n",
                "    save_interval=900  # 15 phÃºt\n",
                ")\n",
                "trainer.setup()\n",
                "\n",
                "if RESUME_FROM:\n",
                "    print(f\"\\nðŸ”„ Resuming training from: {RESUME_FROM}\")\n",
                "    trainer.train(resume_from=RESUME_FROM)\n",
                "else:\n",
                "    print(\"\\nðŸš€ Starting training from scratch\")\n",
                "    trainer.train()\n",
                "\n",
                "# LÆ°u checkpoint cuá»‘i cÃ¹ng\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"âœ… Training Complete!\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# LÆ°u best model vÃ o Drive\n",
                "if os.path.exists('checkpoints/best_model.pt'):\n",
                "    shutil.copy('checkpoints/best_model.pt', os.path.join(CHECKPOINT_DIR, 'best_model.pt'))\n",
                "    print(f\"âœ“ Best model saved to: {CHECKPOINT_DIR}/best_model.pt\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 12. Test translations\n",
                "from src.evaluate import load_translator\n",
                "\n",
                "translator = load_translator()\n",
                "\n",
                "test_sentences = [\n",
                "    \"Hello, how are you?\",\n",
                "    \"Thank you very much.\",\n",
                "    \"I love learning new languages.\",\n",
                "    \"The weather is beautiful today.\",\n",
                "    \"What is your name?\"\n",
                "]\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"Translation Results\")\n",
                "print(\"=\"*60)\n",
                "for sentence in test_sentences:\n",
                "    translation = translator.translate(sentence)\n",
                "    print(f\"EN: {sentence}\")\n",
                "    print(f\"VI: {translation}\")\n",
                "    print(\"-\"*40)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 13. Evaluate BLEU score\n",
                "!python -m src.evaluate --checkpoint checkpoints/best_model.pt --split test --max-samples 1000"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 14. Save final model to Google Drive\n",
                "import shutil\n",
                "\n",
                "final_save_path = '/content/drive/MyDrive/EV_Translation_Model_Final'\n",
                "os.makedirs(final_save_path, exist_ok=True)\n",
                "\n",
                "# Copy all model files\n",
                "shutil.copy('checkpoints/best_model.pt', final_save_path)\n",
                "shutil.copytree('checkpoints/tokenizers', f'{final_save_path}/tokenizers', dirs_exist_ok=True)\n",
                "shutil.copy('config/config.yaml', final_save_path)\n",
                "\n",
                "print(f\"âœ“ Final model saved to {final_save_path}\")\n",
                "print(\"\\nFiles saved:\")\n",
                "for f in os.listdir(final_save_path):\n",
                "    print(f\"  - {f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## ðŸ“‹ CÃ¡ch Resume Training\n",
                "\n",
                "Náº¿u Colab bá»‹ ngáº¯t, chá»‰ cáº§n **cháº¡y láº¡i tá»« cell Ä‘áº§u tiÃªn**. Há»‡ thá»‘ng sáº½:\n",
                "1. Mount láº¡i Google Drive\n",
                "2. Tá»± Ä‘á»™ng tÃ¬m checkpoint trong Drive\n",
                "3. Resume training tá»« checkpoint Ä‘Ã³\n",
                "\n",
                "**KhÃ´ng cáº§n lÃ m gÃ¬ thÃªm!** ðŸŽ‰\n",
                "\n",
                "---\n",
                "\n",
                "## ðŸ’¡ Tips\n",
                "\n",
                "1. **Colab Pro** gives longer runtime and better GPUs (A100)\n",
                "2. Checkpoint Ä‘Æ°á»£c lÆ°u vÃ o Drive má»—i 15 phÃºt\n",
                "3. Náº¿u muá»‘n reset vÃ  train láº¡i tá»« Ä‘áº§u, xÃ³a folder `EV_Translation_Checkpoint` trong Drive"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}