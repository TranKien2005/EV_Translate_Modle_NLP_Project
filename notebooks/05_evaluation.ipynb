{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496969c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sacrebleu import corpus_bleu\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70071bd",
   "metadata": {},
   "source": [
    "## 1. Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a49f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from pathlib import Path\n",
    "project_root = Path('..')\n",
    "\n",
    "import torch\n",
    "from src.evaluate import load_tokenizers_and_config, build_and_load_model, load_test_split\n",
    "\n",
    "# Load tokenizers and config\n",
    "tokenizer_info, sp_vi, sp_en = load_tokenizers_and_config(project_root)\n",
    "\n",
    "# Load model (checkpoint path can be modified)\n",
    "ckpt = '../checkpoints/current_checkpoint1.pt'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = build_and_load_model(ckpt, tokenizer_info, device=device)\n",
    "model.to(device)\n",
    "print('Model loaded on', device)\n",
    "print('Tokenizer max length:', tokenizer_info['max_length'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc3f9d1",
   "metadata": {},
   "source": [
    "## 2. Greedy Search Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925c2c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluate import greedy_decode\n",
    "\n",
    "def greedy_decode_sentence(model, sentence, sp_src, sp_tgt, tokenizer_info, device, max_len=None):\n",
    "    \"\"\"Tokenize `sentence`, run greedy decode and return detokenized output.\"\"\"\n",
    "    src_ids = sp_src.encode_as_ids(sentence)\n",
    "    src_ids = [tokenizer_info['bos_id']] + src_ids + [tokenizer_info['eos_id']]\n",
    "    if max_len is None:\n",
    "        max_len = tokenizer_info.get('max_length', 128)\n",
    "    if len(src_ids) > max_len:\n",
    "        src_ids = src_ids[:max_len-1] + [tokenizer_info['eos_id']]\n",
    "    return greedy_decode(model, src_ids, sp_tgt, tokenizer_info, device, max_len=max_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720bc2b1",
   "metadata": {},
   "source": [
    "## 3. Beam Search Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e3a682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def beam_search_decode_sentence(model, sentence, sp_src, sp_tgt, tokenizer_info, device, beam_size=5, max_len=None):\n",
    "    \"\"\"Simple beam search implementation returning detokenized best hypothesis.\"\"\"\n",
    "    if max_len is None:\n",
    "        max_len = tokenizer_info.get('max_length', 128)\n",
    "\n",
    "    src_ids = sp_src.encode_as_ids(sentence)\n",
    "    src_ids = [tokenizer_info['bos_id']] + src_ids + [tokenizer_info['eos_id']]\n",
    "    if len(src_ids) > max_len:\n",
    "        src_ids = src_ids[:max_len-1] + [tokenizer_info['eos_id']]\n",
    "\n",
    "    src = torch.tensor(src_ids, dtype=torch.long, device=device).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        encoder_output, src_mask = model.encode(src)\n",
    "\n",
    "    bos = tokenizer_info['bos_id']\n",
    "    eos = tokenizer_info['eos_id']\n",
    "\n",
    "    beams = [([bos], 0.0)]\n",
    "    for _ in range(max_len):\n",
    "        candidates = []\n",
    "        for seq, score in beams:\n",
    "            if seq[-1] == eos:\n",
    "                candidates.append((seq, score))\n",
    "                continue\n",
    "            tgt = torch.tensor(seq, dtype=torch.long, device=device).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                out = model.decode(tgt, encoder_output, src_mask)\n",
    "            logits = out[0, -1, :]\n",
    "            log_probs = torch.log_softmax(logits, dim=-1)\n",
    "            topk = torch.topk(log_probs, beam_size)\n",
    "            for k in range(topk.values.size(0)):\n",
    "                nid = int(topk.indices[k].item())\n",
    "                nscore = score + float(topk.values[k].item())\n",
    "                candidates.append((seq + [nid], nscore))\n",
    "        beams = sorted(candidates, key=lambda x: x[1], reverse=True)[:beam_size]\n",
    "        if all(b[0][-1] == eos for b in beams):\n",
    "            break\n",
    "\n",
    "    best_seq = beams[0][0]\n",
    "    # strip BOS and EOS\n",
    "    ids = [i for i in best_seq if i != bos]\n",
    "    if ids and ids[-1] == eos:\n",
    "        ids = ids[:-1]\n",
    "\n",
    "    # robust decode\n",
    "    try:\n",
    "        text = sp_tgt.decode_ids(ids)\n",
    "    except Exception:\n",
    "        try:\n",
    "            text = sp_tgt.DecodeIds(ids)\n",
    "        except Exception:\n",
    "            pieces = [sp_tgt.IdToPiece(int(i)) for i in ids] if hasattr(sp_tgt, 'IdToPiece') else []\n",
    "            text = ''.join(pieces).replace('▁', ' ').strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559e8c89",
   "metadata": {},
   "source": [
    "## 4. Translation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbd4739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model, sentence, src_tokenizer, tgt_tokenizer, device, method='beam', beam_size=5, max_len=None):\n",
    "    \"\"\"Translate a single sentence using `method` ('greedy' or 'beam').\"\"\"\n",
    "    if method == 'greedy':\n",
    "        return greedy_decode_sentence(model, sentence, src_tokenizer, tgt_tokenizer, tokenizer_info, device, max_len=max_len)\n",
    "    elif method == 'beam':\n",
    "        return beam_search_decode_sentence(model, sentence, src_tokenizer, tgt_tokenizer, tokenizer_info, device, beam_size=beam_size, max_len=max_len)\n",
    "    else:\n",
    "        raise ValueError('method must be \"greedy\" or \"beam\"')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1f4f86",
   "metadata": {},
   "source": [
    "## 5. BLEU Score Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8249f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacrebleu import corpus_bleu\n",
    "\n",
    "\n",
    "def calculate_bleu(model, test_data, src_tokenizer, tgt_tokenizer, device, method='beam', beam_size=5, max_examples=None):\n",
    "    \"\"\"Translate all examples in `test_data` and compute corpus BLEU.\"\"\"\n",
    "    hyps = []\n",
    "    refs = []\n",
    "    for i, item in enumerate(test_data):\n",
    "        if max_examples is not None and i >= max_examples:\n",
    "            break\n",
    "        src = item.get('vi', item.get('src', ''))\n",
    "        ref = item.get('en', item.get('tgt', ''))\n",
    "        hyp = translate(model, src, src_tokenizer, tgt_tokenizer, device, method=method, beam_size=beam_size)\n",
    "        hyps.append(hyp)\n",
    "        refs.append(ref)\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f\"Translated {i+1}/{len(test_data)}\")\n",
    "    bleu = corpus_bleu(hyps, [refs])\n",
    "    return bleu.score, hyps, refs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e6b6ae",
   "metadata": {},
   "source": [
    "## 6. Quantitative Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b5a287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantitative evaluation: BLEU with greedy and beam (200 samples)\n",
    "\n",
    "if 'test_data' not in globals():\n",
    "    test_data = load_test_split(project_root)\n",
    "\n",
    "light_test = test_data[:200]\n",
    "\n",
    "print('Evaluating (greedy) on 200 samples...')\n",
    "greedy_bleu, greedy_hyps, greedy_refs = calculate_bleu(model, light_test, sp_vi, sp_en, device, method='greedy', max_examples=None)\n",
    "print(f'Greedy BLEU (200): {greedy_bleu:.2f}\\n')\n",
    "\n",
    "print('Evaluating (beam size=5) on 200 samples...')\n",
    "beam_bleu, beam_hyps, beam_refs = calculate_bleu(model, light_test, sp_vi, sp_en, device, method='beam', beam_size=5, max_examples=None)\n",
    "print(f'Beam BLEU (200, beam=5): {beam_bleu:.2f}\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ca8c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display N examples (SRC / REF / GREEDY / BEAM)\n",
    "N = 10\n",
    "light_test = test_data[:N]\n",
    "print(f\"Showing {N} sample translations:\\n\")\n",
    "\n",
    "# Recompute greedy outputs if missing or incomplete\n",
    "if 'greedy_hyps' not in globals() or len(greedy_hyps) < len(light_test):\n",
    "    print('Greedy outputs missing or incomplete — recomputing greedy predictions...')\n",
    "    greedy_bleu, greedy_hyps, greedy_refs = calculate_bleu(model, light_test, sp_vi, sp_en, device, method='greedy', max_examples=None)\n",
    "\n",
    "# Recompute beam outputs if missing or incomplete\n",
    "if 'beam_hyps' not in globals() or len(beam_hyps) < len(light_test):\n",
    "    print('Beam outputs missing or incomplete — computing beam predictions...')\n",
    "    beam_bleu, beam_hyps, beam_refs = calculate_bleu(model, light_test, sp_vi, sp_en, device, method='beam', beam_size=5, max_examples=None)\n",
    "\n",
    "for i in range(min(N, len(light_test))):\n",
    "    src = light_test[i].get('vi', light_test[i].get('src', ''))\n",
    "    ref = greedy_refs[i] if i < len(greedy_refs) else (beam_refs[i] if i < len(beam_refs) else '')\n",
    "    greedy_out = greedy_hyps[i] if i < len(greedy_hyps) else ''\n",
    "    beam_out = beam_hyps[i] if i < len(beam_hyps) else ''\n",
    "    print(f\"Example {i+1}\")\n",
    "    print('SRC   :', src)\n",
    "    print('REF   :', ref)\n",
    "    print('GREEDY:', greedy_out)\n",
    "    print('BEAM  :', beam_out)\n",
    "    print()\n",
    "\n",
    "# Optionally save the samples to a CSV file for further inspection\n",
    "try:\n",
    "    import csv\n",
    "    out_path = project_root / 'results' / 'light_eval_samples.csv'\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(out_path, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['idx', 'src', 'ref', 'greedy', 'beam'])\n",
    "        for i in range(len(light_test)):\n",
    "            src = light_test[i].get('vi', light_test[i].get('src', ''))\n",
    "            ref = greedy_refs[i] if i < len(greedy_refs) else ''\n",
    "            greedy_out = greedy_hyps[i] if i < len(greedy_hyps) else ''\n",
    "            beam_out = beam_hyps[i] if i < len(beam_hyps) else ''\n",
    "            writer.writerow([i, src, ref, greedy_out, beam_out])\n",
    "    print(f\"Saved sample translations to {out_path}\")\n",
    "except Exception as e:\n",
    "    print('Could not save samples:', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d62210",
   "metadata": {},
   "source": [
    "## 7. Qualitative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf42442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Show sample translations\n",
    "# - Good translations\n",
    "# - Bad translations\n",
    "# - Analysis of errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25da63b5",
   "metadata": {},
   "source": [
    "## 8. Attention Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1351da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Visualize attention weights\n",
    "# - Extract attention from model\n",
    "# - Plot attention heatmap\n",
    "# - Analyze which source words affect which target words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2855c29",
   "metadata": {},
   "source": [
    "## 9. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4b38b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Analyze common errors\n",
    "# - Length bias\n",
    "# - Rare word handling\n",
    "# - Grammar errors\n",
    "# - Semantic errors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
