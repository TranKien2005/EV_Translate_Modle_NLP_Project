{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8666a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import json\n",
    "import sentencepiece as spm\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98aa0464",
   "metadata": {},
   "source": [
    "## 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7110db48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading filtered data from JSON...\n",
      "Loaded 350000 sentence pairs\n",
      "\n",
      "First example:\n",
      "VI: C√¢u chuy·ªán b·∫Øt ƒë·∫ßu v·ªõi bu·ªïi l·ªÖ ƒë·∫øm ng∆∞·ª£c .\n",
      "EN: It begins with a countdown .\n"
     ]
    }
   ],
   "source": [
    "# Load filtered dataset from notebook 01\n",
    "print(\"Loading filtered data from JSON...\")\n",
    "with open('../data/raw/phomt_filtered.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(data)} sentence pairs\")\n",
    "print(f\"\\nFirst example:\")\n",
    "print(f\"VI: {data[0]['vi']}\")\n",
    "print(f\"EN: {data[0]['en']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e9be9e",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "344812fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for tokenizer training...\n",
      "‚úÖ Created training files:\n",
      "   ../data/processed/train_vi.txt\n",
      "   ../data/processed/train_en.txt\n"
     ]
    }
   ],
   "source": [
    "# Data is already filtered in notebook 01, no additional cleaning needed\n",
    "# Just prepare text files for SentencePiece training\n",
    "\n",
    "print(\"Preparing data for tokenizer training...\")\n",
    "\n",
    "# Create temporary files for training\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "# Write Vietnamese sentences to temp file\n",
    "vi_train_file = '../data/processed/train_vi.txt'\n",
    "with open(vi_train_file, 'w', encoding='utf-8') as f:\n",
    "    for item in data:\n",
    "        f.write(item['vi'] + '\\n')\n",
    "\n",
    "# Write English sentences to temp file  \n",
    "en_train_file = '../data/processed/train_en.txt'\n",
    "with open(en_train_file, 'w', encoding='utf-8') as f:\n",
    "    for item in data:\n",
    "        f.write(item['en'] + '\\n')\n",
    "\n",
    "print(f\"‚úÖ Created training files:\")\n",
    "print(f\"   {vi_train_file}\")\n",
    "print(f\"   {en_train_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aecca0e",
   "metadata": {},
   "source": [
    "## 3. Tokenization & Vocabulary Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d450674",
   "metadata": {},
   "source": [
    "## 3.5. Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8fbcfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING SENTENCEPIECE TOKENIZERS\n",
      "============================================================\n",
      "\n",
      "[1/2] Training Vietnamese tokenizer...\n",
      "‚úÖ Saved to ../data/processed/spm_vi.model\n",
      "\n",
      "[2/2] Training English tokenizer...\n",
      "‚úÖ Saved to ../data/processed/spm_en.model\n",
      "\n",
      "‚úÖ Vietnamese vocab size: 32000\n",
      "‚úÖ English vocab size: 32000\n",
      "\n",
      "Test tokenization:\n",
      "VI: T√¥i ƒëang h·ªçc ti·∫øng Anh .\n",
      "   Tokens: ['‚ñÅT√¥i', '‚ñÅƒëang', '‚ñÅh·ªçc', '‚ñÅti·∫øng', '‚ñÅAnh', '‚ñÅ.']\n",
      "   IDs: [150, 165, 185, 563, 659, 20]\n",
      "\n",
      "EN: I am learning English .\n",
      "   Tokens: ['‚ñÅI', '‚ñÅam', '‚ñÅlearning', '‚ñÅEnglish', '‚ñÅ.']\n",
      "   IDs: [42, 477, 1563, 2325, 15]\n"
     ]
    }
   ],
   "source": [
    "# Train SentencePiece tokenizers for Vietnamese and English\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING SENTENCEPIECE TOKENIZERS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Configuration\n",
    "vocab_size = 32000\n",
    "model_type = 'bpe'  # or 'unigram'\n",
    "\n",
    "# Train Vietnamese tokenizer\n",
    "print(\"\\n[1/2] Training Vietnamese tokenizer...\")\n",
    "vi_model_prefix = '../data/processed/spm_vi'\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=vi_train_file,\n",
    "    model_prefix=vi_model_prefix,\n",
    "    vocab_size=vocab_size,\n",
    "    model_type=model_type,\n",
    "    character_coverage=0.9995,\n",
    "    pad_id=0,\n",
    "    unk_id=1,\n",
    "    bos_id=2,\n",
    "    eos_id=3,\n",
    "    pad_piece='<pad>',\n",
    "    unk_piece='<unk>',\n",
    "    bos_piece='<s>',\n",
    "    eos_piece='</s>',\n",
    "    user_defined_symbols=['<mask>'],\n",
    "    max_sentence_length=4096\n",
    ")\n",
    "print(f\"‚úÖ Saved to {vi_model_prefix}.model\")\n",
    "\n",
    "# Train English tokenizer\n",
    "print(\"\\n[2/2] Training English tokenizer...\")\n",
    "en_model_prefix = '../data/processed/spm_en'\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=en_train_file,\n",
    "    model_prefix=en_model_prefix,\n",
    "    vocab_size=vocab_size,\n",
    "    model_type=model_type,\n",
    "    character_coverage=1.0,\n",
    "    pad_id=0,\n",
    "    unk_id=1,\n",
    "    bos_id=2,\n",
    "    eos_id=3,\n",
    "    pad_piece='<pad>',\n",
    "    unk_piece='<unk>',\n",
    "    bos_piece='<s>',\n",
    "    eos_piece='</s>',\n",
    "    user_defined_symbols=['<mask>'],\n",
    "    max_sentence_length=4096\n",
    ")\n",
    "print(f\"‚úÖ Saved to {en_model_prefix}.model\")\n",
    "\n",
    "# Load tokenizers\n",
    "sp_vi = spm.SentencePieceProcessor()\n",
    "sp_vi.load(f'{vi_model_prefix}.model')\n",
    "\n",
    "sp_en = spm.SentencePieceProcessor()\n",
    "sp_en.load(f'{en_model_prefix}.model')\n",
    "\n",
    "print(f\"\\n‚úÖ Vietnamese vocab size: {sp_vi.get_piece_size()}\")\n",
    "print(f\"‚úÖ English vocab size: {sp_en.get_piece_size()}\")\n",
    "\n",
    "# Test tokenization\n",
    "test_vi = \"T√¥i ƒëang h·ªçc ti·∫øng Anh .\"\n",
    "test_en = \"I am learning English .\"\n",
    "print(f\"\\nTest tokenization:\")\n",
    "print(f\"VI: {test_vi}\")\n",
    "print(f\"   Tokens: {sp_vi.encode_as_pieces(test_vi)}\")\n",
    "print(f\"   IDs: {sp_vi.encode_as_ids(test_vi)}\")\n",
    "print(f\"\\nEN: {test_en}\")\n",
    "print(f\"   Tokens: {sp_en.encode_as_pieces(test_en)}\")\n",
    "print(f\"   IDs: {sp_en.encode_as_ids(test_en)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d78cefbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data into train/val/test...\n",
      "Train: 300000 samples\n",
      "Val:   25000 samples\n",
      "Test:  25000 samples\n"
     ]
    }
   ],
   "source": [
    "# Split data: 300K train, 25K val, 25K test\n",
    "print(\"Splitting data into train/val/test...\")\n",
    "\n",
    "train_data = data[:300000]\n",
    "val_data = data[300000:325000]\n",
    "test_data = data[325000:350000]\n",
    "\n",
    "print(f\"Train: {len(train_data)} samples\")\n",
    "print(f\"Val:   {len(val_data)} samples\")\n",
    "print(f\"Test:  {len(test_data)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb558455",
   "metadata": {},
   "source": [
    "## 4. Create PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fec7f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PyTorch datasets...\n",
      "‚úÖ Train dataset: 300000 samples\n",
      "‚úÖ Val dataset:   25000 samples\n",
      "‚úÖ Test dataset:  25000 samples\n",
      "\n",
      "Sample from dataset:\n",
      "Source shape: torch.Size([12])\n",
      "Target shape: torch.Size([9])\n",
      "Source IDs: [2, 1484, 367, 320, 180, 86, 1005, 1853, 2199, 1214]...\n",
      "Target IDs: [2, 200, 4012, 110, 7, 581, 7357, 15, 3]...\n"
     ]
    }
   ],
   "source": [
    "# Create PyTorch Dataset class\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, data, sp_src, sp_tgt, max_length=128):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: List of dicts with 'vi' and 'en' keys\n",
    "            sp_src: SentencePiece processor for source (Vietnamese)\n",
    "            sp_tgt: SentencePiece processor for target (English)\n",
    "            max_length: Maximum sequence length (including BOS/EOS)\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.sp_src = sp_src\n",
    "        self.sp_tgt = sp_tgt\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Special token IDs\n",
    "        self.pad_id = 0\n",
    "        self.bos_id = 2\n",
    "        self.eos_id = 3\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Encode source (Vietnamese): <s> + tokens + </s>\n",
    "        src_ids = self.sp_src.encode_as_ids(item['vi'])\n",
    "        src_ids = [self.bos_id] + src_ids + [self.eos_id]\n",
    "        \n",
    "        # Encode target (English): <s> + tokens + </s>\n",
    "        tgt_ids = self.sp_tgt.encode_as_ids(item['en'])\n",
    "        tgt_ids = [self.bos_id] + tgt_ids + [self.eos_id]\n",
    "        \n",
    "        # Truncate if too long\n",
    "        if len(src_ids) > self.max_length:\n",
    "            src_ids = src_ids[:self.max_length-1] + [self.eos_id]\n",
    "        if len(tgt_ids) > self.max_length:\n",
    "            tgt_ids = tgt_ids[:self.max_length-1] + [self.eos_id]\n",
    "        \n",
    "        return {\n",
    "            'src': torch.tensor(src_ids, dtype=torch.long),\n",
    "            'tgt': torch.tensor(tgt_ids, dtype=torch.long),\n",
    "            'src_text': item['vi'],\n",
    "            'tgt_text': item['en']\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "print(\"Creating PyTorch datasets...\")\n",
    "train_dataset = TranslationDataset(train_data, sp_vi, sp_en, max_length=128)\n",
    "val_dataset = TranslationDataset(val_data, sp_vi, sp_en, max_length=128)\n",
    "test_dataset = TranslationDataset(test_data, sp_vi, sp_en, max_length=128)\n",
    "\n",
    "print(f\"‚úÖ Train dataset: {len(train_dataset)} samples\")\n",
    "print(f\"‚úÖ Val dataset:   {len(val_dataset)} samples\")\n",
    "print(f\"‚úÖ Test dataset:  {len(test_dataset)} samples\")\n",
    "\n",
    "# Test dataset\n",
    "sample = train_dataset[0]\n",
    "print(f\"\\nSample from dataset:\")\n",
    "print(f\"Source shape: {sample['src'].shape}\")\n",
    "print(f\"Target shape: {sample['tgt'].shape}\")\n",
    "print(f\"Source IDs: {sample['src'][:10].tolist()}...\")\n",
    "print(f\"Target IDs: {sample['tgt'][:10].tolist()}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc27f44d",
   "metadata": {},
   "source": [
    "## 5. Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "deca6df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Train loader: 4688 batches\n",
      "‚úÖ Val loader:   391 batches\n",
      "‚úÖ Test loader:  391 batches\n",
      "\n",
      "Testing batch loading...\n",
      "Batch keys: dict_keys(['src', 'tgt', 'src_mask', 'tgt_mask'])\n",
      "Source shape: torch.Size([64, 114])\n",
      "Target shape: torch.Size([64, 70])\n",
      "Source mask shape: torch.Size([64, 114])\n",
      "Target mask shape: torch.Size([64, 70])\n"
     ]
    }
   ],
   "source": [
    "# Create collate function for dynamic padding\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate function to pad sequences dynamically\n",
    "    Returns:\n",
    "        src: [batch_size, max_src_len]\n",
    "        tgt: [batch_size, max_tgt_len]\n",
    "        src_mask: [batch_size, max_src_len]\n",
    "        tgt_mask: [batch_size, max_tgt_len]\n",
    "    \"\"\"\n",
    "    src_batch = [item['src'] for item in batch]\n",
    "    tgt_batch = [item['tgt'] for item in batch]\n",
    "    \n",
    "    # Pad sequences\n",
    "    src_padded = pad_sequence(src_batch, batch_first=True, padding_value=0)\n",
    "    tgt_padded = pad_sequence(tgt_batch, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Create masks (1 for real tokens, 0 for padding)\n",
    "    src_mask = (src_padded != 0).long()\n",
    "    tgt_mask = (tgt_padded != 0).long()\n",
    "    \n",
    "    return {\n",
    "        'src': src_padded,\n",
    "        'tgt': tgt_padded,\n",
    "        'src_mask': src_mask,\n",
    "        'tgt_mask': tgt_mask\n",
    "    }\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 64  # Adjust based on GPU memory\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0,  # Set to 0 for Windows, can increase on Linux\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Train loader: {len(train_loader)} batches\")\n",
    "print(f\"‚úÖ Val loader:   {len(val_loader)} batches\")\n",
    "print(f\"‚úÖ Test loader:  {len(test_loader)} batches\")\n",
    "\n",
    "# Test batch\n",
    "print(\"\\nTesting batch loading...\")\n",
    "batch = next(iter(train_loader))\n",
    "print(f\"Batch keys: {batch.keys()}\")\n",
    "print(f\"Source shape: {batch['src'].shape}\")\n",
    "print(f\"Target shape: {batch['tgt'].shape}\")\n",
    "print(f\"Source mask shape: {batch['src_mask'].shape}\")\n",
    "print(f\"Target mask shape: {batch['tgt_mask'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7961ccc2",
   "metadata": {},
   "source": [
    "## 6. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b57b893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving processed data...\n",
      "‚úÖ Saved tokenizer_info.json\n",
      "‚úÖ Saved splits.pkl\n",
      "‚úÖ Saved stats.json\n",
      "\n",
      "============================================================\n",
      "PREPROCESSING COMPLETE!\n",
      "============================================================\n",
      "‚úÖ Tokenizers: spm_vi.model, spm_en.model\n",
      "‚úÖ Vocab sizes: VI=32000, EN=32000\n",
      "‚úÖ Data splits: train=300000, val=25000, test=25000\n",
      "‚úÖ DataLoaders ready with batch_size=64\n",
      "\n",
      "üìå Next step: Open 03_model_building.ipynb to test the Transformer model\n"
     ]
    }
   ],
   "source": [
    "# Save processed data and metadata\n",
    "import pickle\n",
    "\n",
    "print(\"Saving processed data...\")\n",
    "\n",
    "# Save tokenizer info\n",
    "tokenizer_info = {\n",
    "    'vi_model': '../data/processed/spm_vi.model',\n",
    "    'en_model': '../data/processed/spm_en.model',\n",
    "    'vi_vocab_size': sp_vi.get_piece_size(),\n",
    "    'en_vocab_size': sp_en.get_piece_size(),\n",
    "    'max_length': 128,\n",
    "    'pad_id': 0,\n",
    "    'unk_id': 1,\n",
    "    'bos_id': 2,\n",
    "    'eos_id': 3,\n",
    "    'special_tokens': ['<pad>', '<unk>', '<s>', '</s>', '<mask>']\n",
    "}\n",
    "\n",
    "with open('../data/processed/tokenizer_info.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(tokenizer_info, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Saved tokenizer_info.json\")\n",
    "\n",
    "# Save split data (optional - for quick loading without re-tokenizing)\n",
    "splits = {\n",
    "    'train': train_data,\n",
    "    'val': val_data,\n",
    "    'test': test_data\n",
    "}\n",
    "\n",
    "with open('../data/processed/splits.pkl', 'wb') as f:\n",
    "    pickle.dump(splits, f)\n",
    "\n",
    "print(\"‚úÖ Saved splits.pkl\")\n",
    "\n",
    "# Save dataset statistics\n",
    "stats = {\n",
    "    'total_samples': len(data),\n",
    "    'train_samples': len(train_data),\n",
    "    'val_samples': len(val_data),\n",
    "    'test_samples': len(test_data),\n",
    "    'batch_size': batch_size,\n",
    "    'train_batches': len(train_loader),\n",
    "    'val_batches': len(val_loader),\n",
    "    'test_batches': len(test_loader)\n",
    "}\n",
    "\n",
    "with open('../data/processed/stats.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(stats, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Saved stats.json\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PREPROCESSING COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"‚úÖ Tokenizers: spm_vi.model, spm_en.model\")\n",
    "print(f\"‚úÖ Vocab sizes: VI={sp_vi.get_piece_size()}, EN={sp_en.get_piece_size()}\")\n",
    "print(f\"‚úÖ Data splits: train={len(train_data)}, val={len(val_data)}, test={len(test_data)}\")\n",
    "print(f\"‚úÖ DataLoaders ready with batch_size={batch_size}\")\n",
    "print(f\"\\nüìå Next step: Open 03_model_building.ipynb to test the Transformer model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
