{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a745db68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully imported all model components!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "\n",
    "# Load model components from src/\n",
    "from src.model import Transformer, PositionalEncoding\n",
    "from src.layers import EncoderLayer, DecoderLayer, FeedForward\n",
    "from src.attention import MultiHeadAttention, scaled_dot_product_attention\n",
    "\n",
    "print(\"‚úÖ Successfully imported all model components!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5934ee1c",
   "metadata": {},
   "source": [
    "## 1. Scaled Dot-Product Attention\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad054d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing scaled_dot_product_attention...\n",
      "‚úÖ Output shape: torch.Size([2, 8, 10, 64])\n",
      "‚úÖ Attention weights shape: torch.Size([2, 8, 10, 10])\n",
      "‚úÖ Masked output shape: torch.Size([2, 8, 10, 64])\n"
     ]
    }
   ],
   "source": [
    "# Test scaled_dot_product_attention\n",
    "print(\"Testing scaled_dot_product_attention...\")\n",
    "\n",
    "# Create dummy inputs\n",
    "batch_size = 2\n",
    "num_heads = 8\n",
    "seq_len = 10\n",
    "d_k = 64\n",
    "\n",
    "Q = torch.randn(batch_size, num_heads, seq_len, d_k)\n",
    "K = torch.randn(batch_size, num_heads, seq_len, d_k)\n",
    "V = torch.randn(batch_size, num_heads, seq_len, d_k)\n",
    "\n",
    "# Test without mask\n",
    "output, attn_weights = scaled_dot_product_attention(Q, K, V)\n",
    "print(f\"‚úÖ Output shape: {output.shape}\")\n",
    "print(f\"‚úÖ Attention weights shape: {attn_weights.shape}\")\n",
    "\n",
    "# Test with mask\n",
    "mask = torch.ones(batch_size, 1, 1, seq_len).bool()\n",
    "output_masked, attn_weights_masked = scaled_dot_product_attention(Q, K, V, mask)\n",
    "print(f\"‚úÖ Masked output shape: {output_masked.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80d0093",
   "metadata": {},
   "source": [
    "## 2. Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76e44aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing MultiHeadAttention...\n",
      "‚úÖ Input shape: torch.Size([2, 10, 512])\n",
      "‚úÖ Output shape: torch.Size([2, 10, 512])\n",
      "‚úÖ Parameters: 1,050,624\n"
     ]
    }
   ],
   "source": [
    "# Test MultiHeadAttention\n",
    "print(\"Testing MultiHeadAttention...\")\n",
    "\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "# Create dummy inputs\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "output, attn_weights = mha(x, x, x)\n",
    "\n",
    "print(f\"‚úÖ Input shape: {x.shape}\")\n",
    "print(f\"‚úÖ Output shape: {output.shape}\")\n",
    "print(f\"‚úÖ Parameters: {sum(p.numel() for p in mha.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661daad6",
   "metadata": {},
   "source": [
    "## 3. Positional Encoding\n",
    "\n",
    "$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71a88894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing PositionalEncoding...\n",
      "‚úÖ Input shape: torch.Size([2, 10, 512])\n",
      "‚úÖ Output shape: torch.Size([2, 10, 512])\n",
      "‚úÖ First 5 positional encoding values:\n",
      "tensor([[ 0.0000,  1.0000,  0.0000,  1.0000,  0.0000],\n",
      "        [ 0.8415,  0.5403,  0.8219,  0.5697,  0.8020],\n",
      "        [ 0.9093, -0.4161,  0.9364, -0.3509,  0.9581],\n",
      "        [ 0.1411, -0.9900,  0.2451, -0.9695,  0.3428],\n",
      "        [-0.7568, -0.6536, -0.6572, -0.7537, -0.5486]])\n"
     ]
    }
   ],
   "source": [
    "# Test PositionalEncoding\n",
    "print(\"Testing PositionalEncoding...\")\n",
    "\n",
    "d_model = 512\n",
    "max_len = 128\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "\n",
    "pe = PositionalEncoding(d_model, max_len)\n",
    "\n",
    "# Create dummy input\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "output = pe(x)\n",
    "\n",
    "print(f\"‚úÖ Input shape: {x.shape}\")\n",
    "print(f\"‚úÖ Output shape: {output.shape}\")\n",
    "print(f\"‚úÖ First 5 positional encoding values:\")\n",
    "print(pe.pe[0, :5, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fd2e2a",
   "metadata": {},
   "source": [
    "## 4. Feed-Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "612324c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing FeedForward Network...\n",
      "‚úÖ Input shape: torch.Size([2, 10, 512])\n",
      "‚úÖ Output shape: torch.Size([2, 10, 512])\n",
      "‚úÖ Parameters: 2,099,712\n"
     ]
    }
   ],
   "source": [
    "# Test FeedForward\n",
    "print(\"Testing FeedForward Network...\")\n",
    "\n",
    "d_model = 512\n",
    "d_ff = 2048\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "\n",
    "ffn = FeedForward(d_model, d_ff)\n",
    "\n",
    "# Create dummy input\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "output = ffn(x)\n",
    "\n",
    "print(f\"‚úÖ Input shape: {x.shape}\")\n",
    "print(f\"‚úÖ Output shape: {output.shape}\")\n",
    "print(f\"‚úÖ Parameters: {sum(p.numel() for p in ffn.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994eb87e",
   "metadata": {},
   "source": [
    "## 5. Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "698a937c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing EncoderLayer...\n",
      "‚úÖ Input shape: torch.Size([2, 10, 512])\n",
      "‚úÖ Output shape: torch.Size([2, 10, 512])\n",
      "‚úÖ Parameters: 3,152,384\n"
     ]
    }
   ],
   "source": [
    "# Test EncoderLayer\n",
    "print(\"Testing EncoderLayer...\")\n",
    "\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "\n",
    "encoder_layer = EncoderLayer(d_model, num_heads, d_ff)\n",
    "\n",
    "# Create dummy input\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "output = encoder_layer(x)\n",
    "\n",
    "print(f\"‚úÖ Input shape: {x.shape}\")\n",
    "print(f\"‚úÖ Output shape: {output.shape}\")\n",
    "print(f\"‚úÖ Parameters: {sum(p.numel() for p in encoder_layer.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02bca5f",
   "metadata": {},
   "source": [
    "## 6. Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23cd1cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DecoderLayer...\n",
      "‚úÖ Target shape: torch.Size([2, 8, 512])\n",
      "‚úÖ Encoder output shape: torch.Size([2, 10, 512])\n",
      "‚úÖ Decoder output shape: torch.Size([2, 8, 512])\n",
      "‚úÖ Parameters: 4,204,032\n"
     ]
    }
   ],
   "source": [
    "# Test DecoderLayer\n",
    "print(\"Testing DecoderLayer...\")\n",
    "\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "batch_size = 2\n",
    "src_len = 10\n",
    "tgt_len = 8\n",
    "\n",
    "decoder_layer = DecoderLayer(d_model, num_heads, d_ff)\n",
    "\n",
    "# Create dummy inputs\n",
    "tgt = torch.randn(batch_size, tgt_len, d_model)\n",
    "encoder_output = torch.randn(batch_size, src_len, d_model)\n",
    "\n",
    "output = decoder_layer(tgt, encoder_output)\n",
    "\n",
    "print(f\"‚úÖ Target shape: {tgt.shape}\")\n",
    "print(f\"‚úÖ Encoder output shape: {encoder_output.shape}\")\n",
    "print(f\"‚úÖ Decoder output shape: {output.shape}\")\n",
    "print(f\"‚úÖ Parameters: {sum(p.numel() for p in decoder_layer.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53afdb8",
   "metadata": {},
   "source": [
    "## 7. Full Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f868cd73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer configuration...\n",
      "‚úÖ Source vocab size: 32000\n",
      "‚úÖ Target vocab size: 32000\n",
      "‚úÖ Max length: 128\n",
      "\n",
      "Initializing Transformer model...\n",
      "‚úÖ Model created successfully!\n",
      "\n",
      "üìä Model Statistics:\n",
      "   Total parameters: 93,324,544\n",
      "   Trainable parameters: 93,324,544\n",
      "   Model size: ~356.00 MB (fp32)\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer config\n",
    "print(\"Loading tokenizer configuration...\")\n",
    "with open('../data/processed/tokenizer_info.json', 'r') as f:\n",
    "    tokenizer_info = json.load(f)\n",
    "\n",
    "src_vocab_size = tokenizer_info['vi_vocab_size']\n",
    "tgt_vocab_size = tokenizer_info['en_vocab_size']\n",
    "max_len = tokenizer_info['max_length']\n",
    "\n",
    "print(f\"‚úÖ Source vocab size: {src_vocab_size}\")\n",
    "print(f\"‚úÖ Target vocab size: {tgt_vocab_size}\")\n",
    "print(f\"‚úÖ Max length: {max_len}\")\n",
    "\n",
    "# Initialize Transformer\n",
    "print(\"\\nInitializing Transformer model...\")\n",
    "model = Transformer(\n",
    "    src_vocab_size=src_vocab_size,\n",
    "    tgt_vocab_size=tgt_vocab_size,\n",
    "    d_model=512,\n",
    "    num_heads=8,\n",
    "    num_encoder_layers=6,\n",
    "    num_decoder_layers=6,\n",
    "    d_ff=2048,\n",
    "    max_len=max_len,\n",
    "    dropout=0.1,\n",
    "    pad_idx=tokenizer_info['pad_id']\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Model created successfully!\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nüìä Model Statistics:\")\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"   Model size: ~{total_params * 4 / (1024**2):.2f} MB (fp32)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7037fa5",
   "metadata": {},
   "source": [
    "## 8. Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6e36e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing full Transformer forward pass...\n",
      "Source shape: torch.Size([4, 20])\n",
      "Target shape: torch.Size([4, 15])\n",
      "\n",
      "‚úÖ Output shape: torch.Size([4, 15, 32000])\n",
      "   Expected: [batch_size=4, tgt_len=15, vocab_size=32000]\n",
      "\n",
      "Testing with masks (should fail, do not use extra mask arguments)...\n",
      "‚ùå Error: Transformer.forward() takes 3 positional arguments but 5 were given\n",
      "Do NOT pass masks to model; only pass src and tgt. Masks are handled internally.\n",
      "\n",
      "============================================================\n",
      "MODEL BUILDING COMPLETE!\n",
      "============================================================\n",
      "‚úÖ All components working correctly\n",
      "‚úÖ Model ready for training\n",
      "‚úÖ Total parameters: 93,324,544\n",
      "\n",
      "üìå Next step: Open 04_training.ipynb to train the model\n"
     ]
    }
   ],
   "source": [
    "# Test full model forward pass\n",
    "print(\"Testing full Transformer forward pass...\")\n",
    "\n",
    "batch_size = 4\n",
    "src_len = 20\n",
    "tgt_len = 15\n",
    "\n",
    "# Create dummy inputs (random token IDs)\n",
    "src = torch.randint(0, src_vocab_size, (batch_size, src_len))\n",
    "tgt = torch.randint(0, tgt_vocab_size, (batch_size, tgt_len))\n",
    "\n",
    "print(f\"Source shape: {src.shape}\")\n",
    "print(f\"Target shape: {tgt.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "output = model(src, tgt)\n",
    "\n",
    "print(f\"\\n‚úÖ Output shape: {output.shape}\")\n",
    "print(f\"   Expected: [batch_size={batch_size}, tgt_len={tgt_len}, vocab_size={tgt_vocab_size}]\")\n",
    "\n",
    "# Test with masks (should fail, for demonstration)\n",
    "print(\"\\nTesting with masks (should fail, do not use extra mask arguments)...\")\n",
    "try:\n",
    "    src_mask = (src != tokenizer_info['pad_id'])\n",
    "    tgt_mask = (tgt != tokenizer_info['pad_id'])\n",
    "    output_masked = model(src, tgt, src_mask, tgt_mask)\n",
    "    print(f\"‚úÖ Masked output shape: {output_masked.shape}\")\n",
    "except TypeError as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"Do NOT pass masks to model; only pass src and tgt. Masks are handled internally.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL BUILDING COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"‚úÖ All components working correctly\")\n",
    "print(f\"‚úÖ Model ready for training\")\n",
    "print(f\"‚úÖ Total parameters: {total_params:,}\")\n",
    "print(f\"\\nüìå Next step: Open 04_training.ipynb to train the model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
