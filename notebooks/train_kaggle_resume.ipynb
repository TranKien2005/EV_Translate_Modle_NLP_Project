{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üîÑ Resume Training: English-Vietnamese Model\n",
                "\n",
                "Resume training from existing checkpoint.\n",
                "\n",
                "**Prerequisites:**\n",
                "- Checkpoint file (`best_model.pt` ho·∫∑c `checkpoint_epoch_X.pt`)\n",
                "- Tokenizer files (`tokenizer_en.model`, `tokenizer_vi.model`)\n",
                "- Processed data (`train.pt`, `val.pt`)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. ‚öôÔ∏è Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!git clone https://github.com/TranKien2005/EV_Translate_Modle_NLP_Project.git\n",
                "%cd EV_Translate_Modle_NLP_Project"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q datasets sentencepiece sacrebleu google-generativeai python-dotenv tqdm tensorboard pyyaml"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "GEMINI_API_KEY = \"YOUR_GEMINI_API_KEY_HERE\"\n",
                "HF_TOKEN = \"YOUR_HF_TOKEN_HERE\"\n",
                "\n",
                "with open('.env', 'w') as f:\n",
                "    f.write(f'GEMINI_API_KEY={GEMINI_API_KEY}\\n')\n",
                "    f.write(f'HF_TOKEN={HF_TOKEN}\\n')\n",
                "print('‚úì .env created')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. üîß Configure Paths\n",
                "\n",
                "**Nh·∫≠p tr·ª±c ti·∫øp ƒë∆∞·ªùng d·∫´n ƒë·∫øn c√°c files tr√™n Kaggle:**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# üìÅ NH·∫¨P ƒê∆Ø·ªúNG D·∫™N TR·ª∞C TI·∫æP ƒê·∫æN C√ÅC FILES TR√äN KAGGLE\n",
                "# ============================================================\n",
                "\n",
                "# ƒê∆∞·ªùng d·∫´n ƒë·∫øn checkpoint ƒë·ªÉ resume (c√≥ th·ªÉ l√† best_model.pt ho·∫∑c checkpoint_epoch_X.pt)\n",
                "CHECKPOINT_PATH = \"/kaggle/input/YOUR_DATASET/best_model.pt\"  # THAY ƒê·ªîI\n",
                "\n",
                "# ƒê∆∞·ªùng d·∫´n ƒë·∫øn tokenizers\n",
                "TOKENIZER_SRC_PATH = \"/kaggle/input/YOUR_DATASET/tokenizer_en.model\"  # THAY ƒê·ªîI\n",
                "TOKENIZER_TGT_PATH = \"/kaggle/input/YOUR_DATASET/tokenizer_vi.model\"  # THAY ƒê·ªîI\n",
                "\n",
                "# ƒê∆∞·ªùng d·∫´n ƒë·∫øn processed data\n",
                "TRAIN_DATA_PATH = \"/kaggle/input/YOUR_DATASET/train.pt\"  # THAY ƒê·ªîI\n",
                "VAL_DATA_PATH = \"/kaggle/input/YOUR_DATASET/val.pt\"  # THAY ƒê·ªîI\n",
                "\n",
                "# Output directory (n∆°i l∆∞u checkpoints m·ªõi)\n",
                "OUTPUT_CHECKPOINT_DIR = \"/kaggle/working/checkpoints\"\n",
                "OUTPUT_LOG_DIR = \"/kaggle/working/logs\"\n",
                "\n",
                "# ============================================================\n",
                "print(\"üìÅ Input Paths:\")\n",
                "print(f\"  üì¶ Checkpoint: {CHECKPOINT_PATH}\")\n",
                "print(f\"  üìù Tokenizer EN: {TOKENIZER_SRC_PATH}\")\n",
                "print(f\"  üìù Tokenizer VI: {TOKENIZER_TGT_PATH}\")\n",
                "print(f\"  üìä Train data: {TRAIN_DATA_PATH}\")\n",
                "print(f\"  üìä Val data: {VAL_DATA_PATH}\")\n",
                "print(f\"\\nüìÅ Output Paths:\")\n",
                "print(f\"  üíæ Checkpoints: {OUTPUT_CHECKPOINT_DIR}\")\n",
                "print(f\"  üìà Logs: {OUTPUT_LOG_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Verify input files exist\n",
                "import os\n",
                "\n",
                "files_to_check = [\n",
                "    (\"Checkpoint\", CHECKPOINT_PATH),\n",
                "    (\"Tokenizer EN\", TOKENIZER_SRC_PATH),\n",
                "    (\"Tokenizer VI\", TOKENIZER_TGT_PATH),\n",
                "    (\"Train data\", TRAIN_DATA_PATH),\n",
                "    (\"Val data\", VAL_DATA_PATH),\n",
                "]\n",
                "\n",
                "all_exist = True\n",
                "for name, path in files_to_check:\n",
                "    exists = os.path.exists(path)\n",
                "    status = \"‚úì\" if exists else \"‚úó\"\n",
                "    print(f\"{status} {name}: {path}\")\n",
                "    if not exists:\n",
                "        all_exist = False\n",
                "\n",
                "if all_exist:\n",
                "    print(\"\\n‚úÖ All input files found!\")\n",
                "else:\n",
                "    print(\"\\n‚ùå Some files not found. Please check the paths above.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. üìÇ Setup Directories & Symlinks"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import shutil\n",
                "\n",
                "# Create output directories\n",
                "os.makedirs(OUTPUT_CHECKPOINT_DIR, exist_ok=True)\n",
                "os.makedirs(f\"{OUTPUT_CHECKPOINT_DIR}/tokenizers\", exist_ok=True)\n",
                "os.makedirs(OUTPUT_LOG_DIR, exist_ok=True)\n",
                "os.makedirs(\"/kaggle/working/data/processed\", exist_ok=True)\n",
                "\n",
                "# Copy tokenizers to output dir (needed for saving)\n",
                "shutil.copy(TOKENIZER_SRC_PATH, f\"{OUTPUT_CHECKPOINT_DIR}/tokenizers/tokenizer_en.model\")\n",
                "shutil.copy(TOKENIZER_TGT_PATH, f\"{OUTPUT_CHECKPOINT_DIR}/tokenizers/tokenizer_vi.model\")\n",
                "\n",
                "# Create symlinks for data (avoid copying large files)\n",
                "if not os.path.exists(\"/kaggle/working/data/processed/train.pt\"):\n",
                "    os.symlink(TRAIN_DATA_PATH, \"/kaggle/working/data/processed/train.pt\")\n",
                "if not os.path.exists(\"/kaggle/working/data/processed/val.pt\"):\n",
                "    os.symlink(VAL_DATA_PATH, \"/kaggle/working/data/processed/val.pt\")\n",
                "\n",
                "print(\"‚úì Directories created\")\n",
                "print(\"‚úì Tokenizers copied\")\n",
                "print(\"‚úì Data symlinks created\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Update config with paths\n",
                "import yaml\n",
                "\n",
                "CONFIG_FILE = 'config/config.yaml'\n",
                "\n",
                "with open(CONFIG_FILE, 'r') as f:\n",
                "    cfg = yaml.safe_load(f)\n",
                "\n",
                "cfg['paths'] = {\n",
                "    'data_dir': '/kaggle/working/data',\n",
                "    'checkpoint_dir': OUTPUT_CHECKPOINT_DIR,\n",
                "    'log_dir': OUTPUT_LOG_DIR\n",
                "}\n",
                "cfg['data']['source'] = 'processed'\n",
                "\n",
                "with open(CONFIG_FILE, 'w') as f:\n",
                "    yaml.dump(cfg, f, default_flow_style=False, allow_unicode=True)\n",
                "\n",
                "print('‚úì Config updated')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. üîç Verify Checkpoint"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "sys.path.insert(0, '.')\n",
                "\n",
                "import torch\n",
                "\n",
                "# Load and inspect checkpoint\n",
                "checkpoint = torch.load(CHECKPOINT_PATH, map_location='cpu')\n",
                "\n",
                "print(\"üì¶ Checkpoint Info:\")\n",
                "print(f\"  Epoch: {checkpoint.get('epoch', 'N/A')}\")\n",
                "print(f\"  Val Loss: {checkpoint.get('val_loss', 'N/A'):.4f}\" if 'val_loss' in checkpoint else \"  Val Loss: N/A\")\n",
                "print(f\"  Best Val Loss: {checkpoint.get('best_val_loss', 'N/A'):.4f}\" if 'best_val_loss' in checkpoint else \"  Best Val Loss: N/A\")\n",
                "print(f\"  Has scheduler state: {'scheduler_state_dict' in checkpoint}\")\n",
                "print(f\"  Keys: {list(checkpoint.keys())}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. üèãÔ∏è Resume Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.train import Trainer\n",
                "from src.config import load_config\n",
                "\n",
                "config = load_config('config/config.yaml')\n",
                "trainer = Trainer(config_path='config/config.yaml')\n",
                "trainer.setup()\n",
                "\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"üîÑ Resuming EN ‚Üí VI Training\")\n",
                "print(\"=\"*50)\n",
                "\n",
                "# Resume t·ª´ checkpoint ƒë√£ ch·ªâ ƒë·ªãnh\n",
                "trainer.train(resume_from=CHECKPOINT_PATH)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}