{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ§ª Test Notebook - 100K Samples\n",
                "\n",
                "Quick test with 100K samples and 2 epochs (~30 min)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Clone repo\n",
                "!rm -rf EV_Translate_Modle_NLP_Project\n",
                "!git clone https://github.com/TranKien2005/EV_Translate_Modle_NLP_Project.git\n",
                "%cd EV_Translate_Modle_NLP_Project"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Install deps (skip torch)\n",
                "!pip install -q datasets sentencepiece sacrebleu google-generativeai python-dotenv tqdm tensorboard seaborn pyyaml"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Create .env with HF token\n",
                "HF_TOKEN = \"YOUR_HF_TOKEN\"  # <<< REPLACE THIS!\n",
                "\n",
                "with open('.env', 'w') as f:\n",
                "    f.write(f'HF_TOKEN={HF_TOKEN}\\n')\n",
                "print('âœ“ .env created')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Download PhoMT\n",
                "!python scripts/download_phomt.py"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. Verify download & check paths\n",
                "from src.config import load_config\n",
                "config = load_config()\n",
                "\n",
                "print(f\"data_dir: {config.paths.data_dir}\")\n",
                "!ls -la {config.paths.data_dir}/PhoMT/detokenization/ 2>/dev/null || echo \"Path not found!\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6. Preprocess (100K only)\n",
                "!python scripts/preprocess_data.py --max-samples 100000"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 7. Setup for quick training\n",
                "import yaml\n",
                "\n",
                "with open('config/config.yaml', 'r') as f:\n",
                "    cfg = yaml.safe_load(f)\n",
                "\n",
                "cfg['data']['source'] = 'processed'\n",
                "cfg['training']['epochs'] = 2\n",
                "\n",
                "with open('config/config.yaml', 'w') as f:\n",
                "    yaml.dump(cfg, f, default_flow_style=False)\n",
                "\n",
                "print('âœ“ Config: 2 epochs, processed data')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 8. Train!\n",
                "from src.train import Trainer\n",
                "\n",
                "trainer = Trainer()\n",
                "trainer.setup()\n",
                "trainer.train()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 9. Test translations\n",
                "from src.evaluate import load_translator\n",
                "\n",
                "translator = load_translator(\n",
                "    'checkpoints/best_model.pt',\n",
                "    'checkpoints/tokenizers/tokenizer_src.model',\n",
                "    'checkpoints/tokenizers/tokenizer_tgt.model',\n",
                "    'config/config.yaml'\n",
                ")\n",
                "\n",
                "for s in [\"Hello\", \"How are you?\", \"Thank you\"]:\n",
                "    print(f\"EN: {s} -> VI: {translator.translate(s)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## âœ… If you see translations, it works!\n",
                "\n",
                "Use `train_kaggle.ipynb` for full training."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}