{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üß™ Test Notebook - 100K Samples Only\n",
                "\n",
                "**Quick test to verify everything works before full training**\n",
                "\n",
                "- Uses only **100,000 samples** instead of ~3M\n",
                "- Runs **2 epochs** only\n",
                "- Should complete in **~30 minutes**\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. ‚öôÔ∏è Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clone repository\n",
                "!rm -rf EV_Translate_Modle_NLP_Project\n",
                "!git clone https://github.com/TranKien2005/EV_Translate_Modle_NLP_Project.git\n",
                "%cd EV_Translate_Modle_NLP_Project"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies (skip torch - already on Kaggle)\n",
                "!pip install -q datasets sentencepiece sacrebleu google-generativeai python-dotenv tqdm tensorboard seaborn pyyaml"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Verify PyTorch + CUDA\n",
                "import torch\n",
                "print(f'PyTorch: {torch.__version__}')\n",
                "print(f'CUDA: {torch.cuda.is_available()}')\n",
                "if torch.cuda.is_available():\n",
                "    print(f'Device: {torch.cuda.get_device_name(0)}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create .env (replace with your keys)\n",
                "GEMINI_API_KEY = \"YOUR_GEMINI_API_KEY\"  # Optional\n",
                "HF_TOKEN = \"YOUR_HF_TOKEN\"  # Get from huggingface.co\n",
                "\n",
                "with open('.env', 'w') as f:\n",
                "    f.write(f'GEMINI_API_KEY={GEMINI_API_KEY}\\n')\n",
                "    f.write(f'HF_TOKEN={HF_TOKEN}\\n')\n",
                "print('‚úì .env created')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. üì• Download Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Download PhoMT dataset\n",
                "!python scripts/download_phomt.py"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. üîß Preprocess (100K samples only)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Preprocess with ONLY 100K samples for quick test\n",
                "!python scripts/preprocess_data.py --max-samples 100000"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Verify processed data exists\n",
                "from src.config import load_config\n",
                "config = load_config()\n",
                "print(f\"Data dir: {config.paths.data_dir}\")\n",
                "!ls -la {config.paths.data_dir}/processed/"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. üèãÔ∏è Quick Training (2 epochs)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Modify config for quick test: 2 epochs only\n",
                "import yaml\n",
                "\n",
                "with open('config/config.yaml', 'r') as f:\n",
                "    cfg = yaml.safe_load(f)\n",
                "\n",
                "# Quick test settings\n",
                "cfg['data']['source'] = 'processed'\n",
                "cfg['training']['epochs'] = 2  # Only 2 epochs for test\n",
                "cfg['training']['save_every'] = 1\n",
                "\n",
                "with open('config/config.yaml', 'w') as f:\n",
                "    yaml.dump(cfg, f, default_flow_style=False)\n",
                "\n",
                "print('‚úì Config updated for quick test')\n",
                "print(f\"  - epochs: 2\")\n",
                "print(f\"  - source: processed\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Start training!\n",
                "from src.train import Trainer\n",
                "\n",
                "trainer = Trainer()\n",
                "trainer.setup()\n",
                "trainer.train()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. üìä Quick Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load trained model\n",
                "from src.evaluate import load_translator\n",
                "\n",
                "translator = load_translator(\n",
                "    checkpoint_path='checkpoints/best_model.pt',\n",
                "    vocab_src_path='checkpoints/tokenizers/tokenizer_src.model',\n",
                "    vocab_tgt_path='checkpoints/tokenizers/tokenizer_tgt.model',\n",
                "    config_path='config/config.yaml'\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test translations\n",
                "test_sentences = [\n",
                "    \"Hello, how are you?\",\n",
                "    \"I love you.\",\n",
                "    \"What is your name?\",\n",
                "    \"The weather is nice today.\",\n",
                "    \"Thank you very much.\"\n",
                "]\n",
                "\n",
                "print(\"=\"*50)\n",
                "print(\"üåê Translation Test\")\n",
                "print(\"=\"*50)\n",
                "\n",
                "for sentence in test_sentences:\n",
                "    # Greedy (fast)\n",
                "    greedy = translator.translate(sentence, beam_size=1)\n",
                "    # Beam search (better)\n",
                "    beam = translator.translate(sentence, beam_size=4)\n",
                "    \n",
                "    print(f\"\\nüîπ EN: {sentence}\")\n",
                "    print(f\"   Greedy: {greedy}\")\n",
                "    print(f\"   Beam-4: {beam}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Quick BLEU on 100 samples\n",
                "!python -m src.evaluate \\\n",
                "    --checkpoint checkpoints/best_model.pt \\\n",
                "    --vocab-src checkpoints/tokenizers/tokenizer_src.model \\\n",
                "    --vocab-tgt checkpoints/tokenizers/tokenizer_tgt.model \\\n",
                "    --config config/config.yaml \\\n",
                "    --val"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ‚úÖ Success!\n",
                "\n",
                "If you see translations above, everything works!\n",
                "\n",
                "**Next steps:**\n",
                "1. Use `train_kaggle.ipynb` for full training\n",
                "2. Or increase epochs/samples in this notebook\n",
                "\n",
                "**Expected results with 2 epochs on 100K:**\n",
                "- BLEU: 1-5 (very low, just testing)\n",
                "- Translations may be nonsense (not enough training)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}