{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üöÄ English-Vietnamese Translation Model Training\n",
                "\n",
                "**Transformer-based Neural Machine Translation from Scratch**\n",
                "\n",
                "This notebook trains an EN-VI translation model using:\n",
                "- **PhoMT Dataset** (~3M sentence pairs)\n",
                "- **SentencePiece Tokenization**\n",
                "- **Transformer Architecture** with Pre-LayerNorm\n",
                "- **Beam Search** for inference\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. ‚öôÔ∏è Setup Environment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clone repository\n",
                "!git clone https://github.com/TranKien2005/EV_Translate_Modle_NLP_Project.git\n",
                "%cd EV_Translate_Modle_NLP_Project"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies (skip torch - already installed on Kaggle)\n",
                "# Kaggle already has PyTorch with correct CUDA, don't reinstall!\n",
                "!pip install -q datasets sentencepiece sacrebleu google-generativeai python-dotenv tqdm tensorboard seaborn pyyaml"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Verify PyTorch and CUDA\n",
                "import torch\n",
                "print(f'PyTorch version: {torch.__version__}')\n",
                "print(f'CUDA available: {torch.cuda.is_available()}')\n",
                "if torch.cuda.is_available():\n",
                "    print(f'CUDA device: {torch.cuda.get_device_name(0)}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create .env file with API keys\n",
                "# ‚ö†Ô∏è IMPORTANT: Replace with your actual keys!\n",
                "\n",
                "GEMINI_API_KEY = \"YOUR_GEMINI_API_KEY_HERE\"  # Get from Google AI Studio\n",
                "HF_TOKEN = \"YOUR_HF_TOKEN_HERE\"  # Get from Hugging Face\n",
                "\n",
                "with open('.env', 'w') as f:\n",
                "    f.write(f'GEMINI_API_KEY={GEMINI_API_KEY}\\n')\n",
                "    f.write(f'HF_TOKEN={HF_TOKEN}\\n')\n",
                "\n",
                "print('‚úì .env file created')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. üì• Download & Preprocess Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Download PhoMT dataset (~500MB)\n",
                "!python scripts/download_phomt.py"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Preprocess data: tokenize, filter by length, save to .pt files\n",
                "# This will train SentencePiece tokenizers and process train/val/test sets\n",
                "!python scripts/preprocess_data.py"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. üîß Configuration Check"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "sys.path.insert(0, '.')\n",
                "\n",
                "from src.config import load_config\n",
                "from src.models import Transformer\n",
                "\n",
                "config = load_config()\n",
                "\n",
                "print(\"=\"*50)\n",
                "print(\"üìã Configuration Summary\")\n",
                "print(\"=\"*50)\n",
                "print(f\"\\nüîπ Model:\")\n",
                "print(f\"   d_model: {config.d_model}\")\n",
                "print(f\"   layers: {config.num_encoder_layers} enc + {config.num_decoder_layers} dec\")\n",
                "print(f\"   d_ff: {config.d_ff}\")\n",
                "print(f\"   dropout: {config.dropout}\")\n",
                "print(f\"\\nüîπ Training:\")\n",
                "print(f\"   epochs: {config.epochs}\")\n",
                "print(f\"   batch_size: {config.batch_size} (effective: {config.batch_size * config.gradient_accumulation_steps})\")\n",
                "print(f\"   learning_rate: {config.learning_rate}\")\n",
                "print(f\"   warmup_steps: {config.warmup_steps}\")\n",
                "print(f\"\\nüîπ Data:\")\n",
                "print(f\"   vocab_size: {config.src_vocab_size}\")\n",
                "print(f\"   max_seq_len: {config.max_seq_len}\")\n",
                "print(f\"   data_source: {config.data_source}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Count model parameters\n",
                "model = Transformer(\n",
                "    src_vocab_size=config.src_vocab_size,\n",
                "    tgt_vocab_size=config.tgt_vocab_size,\n",
                "    d_model=config.d_model,\n",
                "    num_heads=config.num_heads,\n",
                "    num_encoder_layers=config.num_encoder_layers,\n",
                "    num_decoder_layers=config.num_decoder_layers,\n",
                "    d_ff=config.d_ff\n",
                ")\n",
                "\n",
                "total_params = sum(p.numel() for p in model.parameters())\n",
                "print(f\"\\nüìä Model Parameters: {total_params:,} ({total_params/1e6:.1f}M)\")\n",
                "print(f\"üì¶ Model Size: ~{total_params * 4 / 1024 / 1024:.1f} MB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. üèãÔ∏è Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Switch to processed data for faster training\n",
                "import yaml\n",
                "\n",
                "with open('config/config.yaml', 'r') as f:\n",
                "    cfg = yaml.safe_load(f)\n",
                "\n",
                "cfg['data']['source'] = 'processed'  # Use pre-tokenized data\n",
                "\n",
                "with open('config/config.yaml', 'w') as f:\n",
                "    yaml.dump(cfg, f, default_flow_style=False)\n",
                "\n",
                "print('‚úì Config updated to use processed data')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ‚ö° Resume Training from Checkpoint (Optional)\n",
                "\n",
                "N·∫øu b·∫°n c√≥ checkpoint t·ª´ l·∫ßn train tr∆∞·ªõc, upload l√™n Kaggle Input v√† nh·∫≠p ƒë∆∞·ªùng d·∫´n b√™n d∆∞·ªõi.\n",
                "\n",
                "**C√°ch s·ª≠ d·ª•ng:**\n",
                "1. Upload file checkpoint (.pt) l√™n Kaggle Dataset\n",
                "2. Add dataset v√†o notebook\n",
                "3. Nh·∫≠p ƒë∆∞·ªùng d·∫´n v√†o √¥ b√™n d∆∞·ªõi (v√≠ d·ª•: `/kaggle/input/my-checkpoint/best_model.pt`)\n",
                "4. N·∫øu ƒë·ªÉ tr·ªëng ho·∫∑c file kh√¥ng t·ªìn t·∫°i ‚Üí train t·ª´ ƒë·∫ßu"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title üìÇ Nh·∫≠p ƒë∆∞·ªùng d·∫´n checkpoint ƒë·ªÉ resume training\n",
                "#@markdown ƒê·ªÉ tr·ªëng n·∫øu mu·ªën train t·ª´ ƒë·∫ßu\n",
                "\n",
                "import os\n",
                "\n",
                "# ========================================\n",
                "# üëá NH·∫¨P ƒê∆Ø·ªúNG D·∫™N CHECKPOINT T·∫†I ƒê√ÇY üëá\n",
                "# ========================================\n",
                "RESUME_CHECKPOINT_PATH = \"\"  # V√≠ d·ª•: \"/kaggle/input/my-model/best_model.pt\"\n",
                "\n",
                "# Ki·ªÉm tra checkpoint\n",
                "if RESUME_CHECKPOINT_PATH and os.path.exists(RESUME_CHECKPOINT_PATH):\n",
                "    print(f\"‚úÖ Checkpoint found: {RESUME_CHECKPOINT_PATH}\")\n",
                "    print(f\"   Size: {os.path.getsize(RESUME_CHECKPOINT_PATH) / 1024 / 1024:.1f} MB\")\n",
                "    RESUME_FROM = RESUME_CHECKPOINT_PATH\n",
                "else:\n",
                "    if RESUME_CHECKPOINT_PATH:\n",
                "        print(f\"‚ö†Ô∏è Checkpoint NOT found: {RESUME_CHECKPOINT_PATH}\")\n",
                "    print(\"üìå Will train from scratch\")\n",
                "    RESUME_FROM = None"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Start training!\n",
                "from src.train import Trainer\n",
                "\n",
                "trainer = Trainer()\n",
                "trainer.setup()\n",
                "\n",
                "# Train v·ªõi ho·∫∑c kh√¥ng c√≥ checkpoint\n",
                "if RESUME_FROM:\n",
                "    print(f\"\\nüîÑ Resuming training from: {RESUME_FROM}\")\n",
                "    trainer.train(resume_from=RESUME_FROM)\n",
                "else:\n",
                "    print(\"\\nüöÄ Starting training from scratch\")\n",
                "    trainer.train()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. üìä Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate on test set with BLEU score\n",
                "!python -m src.evaluate \\\n",
                "    --checkpoint checkpoints/best_model.pt \\\n",
                "    --vocab-src checkpoints/tokenizers/tokenizer_src.model \\\n",
                "    --vocab-tgt checkpoints/tokenizers/tokenizer_tgt.model \\\n",
                "    --config config/config.yaml \\\n",
                "    --test"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate with Gemini Score (optional, requires API key)\n",
                "!python -m src.evaluate \\\n",
                "    --checkpoint checkpoints/best_model.pt \\\n",
                "    --vocab-src checkpoints/tokenizers/tokenizer_src.model \\\n",
                "    --vocab-tgt checkpoints/tokenizers/tokenizer_tgt.model \\\n",
                "    --config config/config.yaml \\\n",
                "    --test --gemini"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. üîÆ Interactive Translation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.evaluate import load_translator\n",
                "from src.config import load_config\n",
                "\n",
                "config = load_config()\n",
                "\n",
                "# Load trained model\n",
                "translator = load_translator(\n",
                "    checkpoint_path='checkpoints/best_model.pt',\n",
                "    vocab_src_path='checkpoints/tokenizers/tokenizer_src.model',\n",
                "    vocab_tgt_path='checkpoints/tokenizers/tokenizer_tgt.model',\n",
                "    config_path='config/config.yaml'\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test translation!\n",
                "test_sentences = [\n",
                "    \"Hello, how are you?\",\n",
                "    \"The weather is nice today.\",\n",
                "    \"I love learning new languages.\",\n",
                "    \"Machine translation is improving rapidly.\",\n",
                "    \"Can you help me with this problem?\"\n",
                "]\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"üåê Translation Examples\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "for sentence in test_sentences:\n",
                "    translation = translator.translate(sentence, beam_size=4)\n",
                "    print(f\"\\nüîπ EN: {sentence}\")\n",
                "    print(f\"üîπ VI: {translation}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. üíæ Save Model to Kaggle Output"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import shutil\n",
                "import os\n",
                "\n",
                "# Copy model files to Kaggle output\n",
                "output_dir = '/kaggle/working/model_output'\n",
                "os.makedirs(output_dir, exist_ok=True)\n",
                "\n",
                "# Copy checkpoint\n",
                "shutil.copy('checkpoints/best_model.pt', output_dir)\n",
                "\n",
                "# Copy tokenizers\n",
                "shutil.copytree('checkpoints/tokenizers', f'{output_dir}/tokenizers', dirs_exist_ok=True)\n",
                "\n",
                "# Copy config\n",
                "shutil.copy('config/config.yaml', output_dir)\n",
                "\n",
                "# Copy evaluation results\n",
                "if os.path.exists('logs'):\n",
                "    shutil.copytree('logs', f'{output_dir}/logs', dirs_exist_ok=True)\n",
                "\n",
                "print(f'\\n‚úì Model saved to {output_dir}')\n",
                "print('\\nFiles:')\n",
                "for f in os.listdir(output_dir):\n",
                "    print(f'  - {f}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üìù Notes\n",
                "\n",
                "**Model Info:**\n",
                "- Architecture: Transformer with Pre-LayerNorm\n",
                "- Parameters: ~18.4M  \n",
                "- Training time: ~8-10 hours on Kaggle GPU\n",
                "\n",
                "**Expected Results:**\n",
                "- BLEU Score: 15-25 (depends on training time)\n",
                "- Gemini Score: 50-70\n",
                "\n",
                "**Tips:**\n",
                "1. Use GPU accelerator (P100/T4) for faster training\n",
                "2. If running out of time, reduce epochs in config\n",
                "3. Save checkpoint frequently to resume if interrupted"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}