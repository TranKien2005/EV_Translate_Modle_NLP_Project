{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aee5d1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f00c243",
   "metadata": {},
   "source": [
    "## 1. Load Model & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3a90d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data and model\n",
    "import json\n",
    "import sentencepiece as spm\n",
    "from src.model import Transformer\n",
    "from src.utils import get_device\n",
    "\n",
    "\n",
    "\n",
    "# Load tokenizer config\n",
    "with open('../data/processed/tokenizer_info.json', 'r') as f:\n",
    "    tokenizer_info = json.load(f)\n",
    "\n",
    "# Load SentencePiece tokenizers\n",
    "sp_vi = spm.SentencePieceProcessor()\n",
    "sp_vi.load(tokenizer_info['vi_model'])\n",
    "sp_en = spm.SentencePieceProcessor()\n",
    "sp_en.load(tokenizer_info['en_model'])\n",
    "\n",
    "# Special token IDs\n",
    "pad_id = tokenizer_info['pad_id']\n",
    "bos_id = tokenizer_info['bos_id']\n",
    "eos_id = tokenizer_info['eos_id']\n",
    "\n",
    "# Load processed splits\n",
    "with open('../data/processed/splits.pkl', 'rb') as f:\n",
    "    splits = pickle.load(f)\n",
    "train_data = splits['train']\n",
    "val_data = splits['val']\n",
    "test_data = splits['test']\n",
    "\n",
    "# Dataset class (reuse from preprocessing)\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, data, sp_src, sp_tgt, max_length=128):\n",
    "        self.data = data\n",
    "        self.sp_src = sp_src\n",
    "        self.sp_tgt = sp_tgt\n",
    "        self.max_length = max_length\n",
    "        self.pad_id = pad_id\n",
    "        self.bos_id = bos_id\n",
    "        self.eos_id = eos_id\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        src_ids = self.sp_src.encode_as_ids(item['vi'])\n",
    "        src_ids = [self.bos_id] + src_ids + [self.eos_id]\n",
    "        tgt_ids = self.sp_tgt.encode_as_ids(item['en'])\n",
    "        tgt_ids = [self.bos_id] + tgt_ids + [self.eos_id]\n",
    "        if len(src_ids) > self.max_length:\n",
    "            src_ids = src_ids[:self.max_length-1] + [self.eos_id]\n",
    "        if len(tgt_ids) > self.max_length:\n",
    "            tgt_ids = tgt_ids[:self.max_length-1] + [self.eos_id]\n",
    "        return {\n",
    "            'src': torch.tensor(src_ids, dtype=torch.long),\n",
    "            'tgt': torch.tensor(tgt_ids, dtype=torch.long),\n",
    "            'src_text': item['vi'],\n",
    "            'tgt_text': item['en']\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_batch = [item['src'] for item in batch]\n",
    "    tgt_batch = [item['tgt'] for item in batch]\n",
    "    src_padded = nn.utils.rnn.pad_sequence(src_batch, batch_first=True, padding_value=pad_id)\n",
    "    tgt_padded = nn.utils.rnn.pad_sequence(tgt_batch, batch_first=True, padding_value=pad_id)\n",
    "    return {\n",
    "        'src': src_padded,\n",
    "        'tgt': tgt_padded\n",
    "    }\n",
    "\n",
    "# Create DataLoaders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ef277e",
   "metadata": {},
   "source": [
    "## 2. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4efc11c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "GPU Memory: 8.59 GB\n",
      "Model loaded on cuda\n",
      "Train samples: 300000 | Val samples: 25000\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "config = {\n",
    "    'd_model': 512,\n",
    "    'num_heads': 8,\n",
    "    'num_encoder_layers': 6,\n",
    "    'num_decoder_layers': 6,\n",
    "    'd_ff': 2048,\n",
    "    'max_len': tokenizer_info['max_length'],\n",
    "    'dropout': 0.1,\n",
    "    'batch_size': 8,\n",
    "    'num_epochs': 5,  # thử 5 epoch trước\n",
    "    'learning_rate': 1e-4,  # Adam lr, NoamScheduler sẽ điều chỉnh lại\n",
    "    'warmup_steps': 4000,\n",
    "    'label_smoothing': 0.1,\n",
    "    'checkpoint_dir': '../checkpoints',\n",
    "    'log_dir': '../logs',\n",
    "}\n",
    "\n",
    "os.makedirs(config['checkpoint_dir'], exist_ok=True)\n",
    "os.makedirs(config['log_dir'], exist_ok=True)\n",
    "\n",
    "batch_size = config['batch_size']\n",
    "train_dataset = TranslationDataset(train_data, sp_vi, sp_en, max_length=tokenizer_info['max_length'])\n",
    "val_dataset = TranslationDataset(val_data, sp_vi, sp_en, max_length=tokenizer_info['max_length'])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=0, pin_memory=True)\n",
    "\n",
    "# Initialize model\n",
    "model = Transformer(\n",
    "    src_vocab_size=tokenizer_info['vi_vocab_size'],\n",
    "    tgt_vocab_size=tokenizer_info['en_vocab_size'],\n",
    "    d_model=512,\n",
    "    num_heads=8,\n",
    "    num_encoder_layers=6,\n",
    "    num_decoder_layers=6,\n",
    "    d_ff=2048,\n",
    "    max_len=tokenizer_info['max_length'],\n",
    "    dropout=0.1,\n",
    "    pad_idx=pad_id\n",
    ")\n",
    "\n",
    "device = get_device()\n",
    "model = model.to(device)\n",
    "print(f\"Model loaded on {device}\")\n",
    "print(f\"Train samples: {len(train_dataset)} | Val samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea464c0",
   "metadata": {},
   "source": [
    "## 3. Loss Function & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d00cb8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup loss function\n",
    "from src.utils import LabelSmoothingLoss\n",
    "\n",
    "criterion = LabelSmoothingLoss(\n",
    "    vocab_size=tokenizer_info['en_vocab_size'],\n",
    "    padding_idx=pad_id,\n",
    "    smoothing=config['label_smoothing']\n",
    ")\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = Adam(model.parameters(), lr=config['learning_rate'], betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a51e191",
   "metadata": {},
   "source": [
    "## 4. Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fee9d7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import NoamScheduler\n",
    "\n",
    "def get_lr_scheduler(optimizer, d_model, warmup_steps):\n",
    "    return NoamScheduler(optimizer, d_model, warmup_steps)\n",
    "\n",
    "scheduler = get_lr_scheduler(optimizer, config['d_model'], config['warmup_steps'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323d23cd",
   "metadata": {},
   "source": [
    "## 5. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "451ba7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, pbar, optimizer, criterion, scheduler, device, checkpoint_dir, grad_accum_steps=1, pad_id=0, epoch=1, resume_step=0):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    current_ckpt_path = os.path.join(checkpoint_dir, 'current_checkpoint.pt')\n",
    "    \n",
    "    # Biến đếm số bước thực tế đã train trong epoch này (để chia trung bình)\n",
    "    step_count = 0 \n",
    "    \n",
    "    for i, batch in enumerate(pbar, 1):\n",
    "        # 1. Logic Skip (Nhanh, nhưng vẫn tốn thời gian load data từ ổ cứng nếu resume sâu)\n",
    "        if i <= resume_step:\n",
    "            continue\n",
    "        \n",
    "        src = batch['src'].to(device)\n",
    "        tgt = batch['tgt'].to(device)\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        tgt_output = tgt[:, 1:].contiguous().view(-1)\n",
    "        \n",
    "        output = model(src, tgt_input)\n",
    "        output = output.contiguous().view(-1, output.size(-1))\n",
    "        \n",
    "        loss = criterion(output, tgt_output) / grad_accum_steps\n",
    "        loss.backward()\n",
    "        \n",
    "        if i % grad_accum_steps == 0 or i == len(pbar):\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # Chỉ gọi .item() 1 lần duy nhất cho loss (Giảm sync GPU-CPU)\n",
    "        # Loss này là Mean Loss của batch (trên valid token)\n",
    "        current_loss = loss.item() * grad_accum_steps\n",
    "        # Đếm số token hợp lệ (không phải padding)\n",
    "        n_tokens = (tgt_output != pad_id).sum().item()\n",
    "        running_loss += current_loss * n_tokens\n",
    "        total_tokens += n_tokens\n",
    "        step_count += 1\n",
    "        \n",
    "        # Hiển thị\n",
    "        pbar.set_postfix({'loss': f'{current_loss:.4f}'})\n",
    "        \n",
    "        # Lưu checkpoint (Dùng current_loss để lưu, nhanh gọn)\n",
    "        if i % 5000 == 0: # Lưu ý: Để 1000 hoặc 5000 thôi, 20000 hơi lâu\n",
    "            save_checkpoint(model, optimizer, epoch, current_loss, current_ckpt_path, step=i)\n",
    "    # Trả về trung bình cộng loss trên số token hợp lệ\n",
    "    return running_loss / total_tokens if total_tokens > 0 else 0\n",
    "\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    step_count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Val', leave=False):\n",
    "            src = batch['src'].to(device)\n",
    "            tgt = batch['tgt'].to(device)\n",
    "            tgt_input = tgt[:, :-1]\n",
    "            tgt_output = tgt[:, 1:].contiguous().view(-1)\n",
    "            output = model(src, tgt_input)\n",
    "            output = output.contiguous().view(-1, output.size(-1))\n",
    "            loss = criterion(output, tgt_output)\n",
    "            n_tokens = (tgt_output != pad_id).sum().item()\n",
    "            running_loss += loss.item() * n_tokens\n",
    "            total_tokens += n_tokens\n",
    "            step_count += 1\n",
    "    return running_loss / total_tokens if total_tokens > 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54be268",
   "metadata": {},
   "source": [
    "## 6. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6195567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Không tìm thấy checkpoint. Bắt đầu train từ đầu (Epoch 1).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 54\u001b[39m\n\u001b[32m     51\u001b[39m pbar = tqdm(train_loader, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, leave=\u001b[38;5;28;01mFalse\u001b[39;00m, initial=step_to_skip)\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# Gọi hàm train_epoch với tham số resume_step\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcheckpoint_dir\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mresume_step\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstep_to_skip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mpad_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_id\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# <--- QUAN TRỌNG: Truyền bước cần skip vào và pad_id\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# QUAN TRỌNG: Sau khi chạy xong epoch dang dở, reset start_step về 0 \u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# để các epoch sau chạy full từ đầu.\u001b[39;00m\n\u001b[32m     62\u001b[39m start_step = \u001b[32m0\u001b[39m \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, pbar, optimizer, criterion, scheduler, device, checkpoint_dir, grad_accum_steps, pad_id, epoch, resume_step)\u001b[39m\n\u001b[32m     31\u001b[39m     optimizer.zero_grad()\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Chỉ gọi .item() 1 lần duy nhất cho loss (Giảm sync GPU-CPU)\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Loss này là Mean Loss của batch (trên valid token)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m current_loss = \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m * grad_accum_steps\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# Đếm số token hợp lệ (không phải padding)\u001b[39;00m\n\u001b[32m     37\u001b[39m n_tokens = (tgt_output != pad_id).sum().item()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from src.utils import save_checkpoint, calculate_perplexity\n",
    "\n",
    "# --- PHẦN 1: LOGIC KHỞI TẠO & RESUME ---\n",
    "resume_path = os.path.join(config['checkpoint_dir'], 'current_checkpoint.pt')\n",
    "\n",
    "# Giá trị mặc định (Train từ đầu)\n",
    "start_epoch = 1\n",
    "start_step = 0\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Kiểm tra nếu có file current -> Load để chạy tiếp\n",
    "if os.path.exists(resume_path):\n",
    "    print(f\"--> Phát hiện file resume: {resume_path}\")\n",
    "    checkpoint = torch.load(resume_path)\n",
    "    \n",
    "    # 1. Load trọng số Model & Optimizer\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    # 2. Cập nhật vị trí epoch và step đang chạy dở\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    start_step = checkpoint.get('step', 0)\n",
    "    \n",
    "    # (Tuỳ chọn) Load lại loss cũ nếu có lưu để so sánh best model\n",
    "    if 'loss' in checkpoint:\n",
    "         print(f\"--> Loss lần cuối: {checkpoint['loss']:.4f}\")\n",
    "\n",
    "    print(f\"--> Hệ thống sẽ RESUME từ Epoch {start_epoch}, Step {start_step}\")\n",
    "else:\n",
    "    print(\"--> Không tìm thấy checkpoint. Bắt đầu train từ đầu (Epoch 1).\")\n",
    "\n",
    "train_losses, val_losses, val_ppls, lrs = [], [], [], []\n",
    "\n",
    "\n",
    "# --- PHẦN 2: VÒNG LẶP CHÍNH (ĐÃ SỬA) ---\n",
    "\n",
    "# Thay vì range(1, ...), ta dùng range(start_epoch, ...)\n",
    "for epoch in range(start_epoch, config['num_epochs'] + 1):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Tính toán step cần bỏ qua (Skip):\n",
    "    # - Nếu là epoch đang chạy dở (start_epoch) -> Skip 'start_step' bước.\n",
    "    # - Nếu là epoch mới hoàn toàn -> Không skip (bằng 0).\n",
    "    step_to_skip = start_step if epoch == start_epoch else 0\n",
    "    \n",
    "    # initial=step_to_skip: Giúp thanh tiến trình hiển thị đúng % ngay khi bắt đầu\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}\", leave=False, initial=step_to_skip)\n",
    "    \n",
    "    # Gọi hàm train_epoch với tham số resume_step\n",
    "    train_loss = train_epoch(model, pbar, optimizer, criterion, scheduler, device, \n",
    "                             checkpoint_dir=config['checkpoint_dir'], \n",
    "                             epoch=epoch,\n",
    "                             resume_step=step_to_skip,\n",
    "                             pad_id=pad_id) # <--- QUAN TRỌNG: Truyền bước cần skip vào và pad_id\n",
    "    \n",
    "    # QUAN TRỌNG: Sau khi chạy xong epoch dang dở, reset start_step về 0 \n",
    "    # để các epoch sau chạy full từ đầu.\n",
    "    start_step = 0 \n",
    "    \n",
    "    # --- PHẦN VALIDATE & SAVE (GIỮ NGUYÊN) ---\n",
    "    val_loss = validate(model, val_loader, criterion, device)\n",
    "    elapsed = time.time() - start_time\n",
    "    perplexity = calculate_perplexity(val_loss)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    val_ppls.append(perplexity)\n",
    "    lrs.append(optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "    # Save checkpoint epoch (Lưu trữ lịch sử)\n",
    "    ckpt_path = os.path.join(config['checkpoint_dir'], f'transformer_epoch_{epoch}.pt')\n",
    "    save_checkpoint(model, optimizer, epoch, val_loss, ckpt_path)\n",
    "    \n",
    "    # Save best model (Lưu model tốt nhất)\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_ckpt_path = os.path.join(config['checkpoint_dir'], 'best_transformer.pt')\n",
    "        save_checkpoint(model, optimizer, epoch, val_loss, best_ckpt_path)\n",
    "    \n",
    "    tqdm.write(f\"Epoch {epoch:2d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val PPL: {perplexity:.2f} | LR: {lrs[-1]:.6f} | Time: {elapsed:.1f}s | Ckpt: {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9f3313",
   "metadata": {},
   "source": [
    "## 7. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b0c1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Plot training curves\n",
    "# - Loss curves (train vs validation)\n",
    "# - Perplexity\n",
    "# - Learning rate schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d1254b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No CUDA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f8bbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.backends.cudnn.enabled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf4ea68",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bc3111",
   "metadata": {},
   "source": [
    "## 8. Debug: Kiểm tra dữ liệu và logits đầu ra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63b0bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số mẫu train: 300000\n",
      "Số mẫu val: 25000\n",
      "Độ dài src trung bình: 23.655416666666667 | max: 135 | min: 3\n",
      "Độ dài tgt trung bình: 20.765993333333334 | max: 132 | min: 3\n",
      "Số câu src rỗng: 0\n",
      "Số câu tgt rỗng: 0\n",
      "Số cặp trùng lặp: 1\n",
      "Một số mẫu train:\n",
      "src: Câu chuyện bắt đầu với buổi lễ đếm ngược .\n",
      "tgt: It begins with a countdown .\n",
      "---\n",
      "src: Ngày 14 , tháng 8 , năm 1947 , gần nửa đêm , ở Bombay , có một phụ nữ sắp lâm bồn .\n",
      "tgt: On August 14th , 1947 , a woman in Bombay goes into labor as the clock ticks towards midnight .\n",
      "---\n",
      "src: Cùng lúc , trên khắp đất Ấn , người ta nín thở chờ đợi tuyên ngôn độc lập sau gần hai thập kỷ là thuộc địa của Anh .\n",
      "tgt: Across India , people hold their breath for the declaration of independence after nearly two centuries of British occupation and rule .\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Kiểm tra dữ liệu train/val: số lượng, độ dài, trùng lặp, rỗng, lỗi token hóa\n",
    "print('Số mẫu train:', len(train_data))\n",
    "print('Số mẫu val:', len(val_data))\n",
    "src_lens = [len(sp_vi.encode_as_ids(item['vi'])) for item in train_data]\n",
    "tgt_lens = [len(sp_en.encode_as_ids(item['en'])) for item in train_data]\n",
    "print('Độ dài src trung bình:', sum(src_lens)/len(src_lens), '| max:', max(src_lens), '| min:', min(src_lens))\n",
    "print('Độ dài tgt trung bình:', sum(tgt_lens)/len(tgt_lens), '| max:', max(tgt_lens), '| min:', min(tgt_lens))\n",
    "print('Số câu src rỗng:', sum([len(item['vi'].strip())==0 for item in train_data]))\n",
    "print('Số câu tgt rỗng:', sum([len(item['en'].strip())==0 for item in train_data]))\n",
    "print('Số cặp trùng lặp:', len(train_data) - len(set((item['vi'], item['en']) for item in train_data)))\n",
    "print('Một số mẫu train:')\n",
    "for i in range(3):\n",
    "    print(f\"src: {train_data[i]['vi']}\")\n",
    "    print(f\"tgt: {train_data[i]['en']}\")\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1846144",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 71) (2732133536.py, line 71)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 71\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mout_path.write_text('\u001b[39m\n                        ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unterminated string literal (detected at line 71)\n"
     ]
    }
   ],
   "source": [
    "# Diagnostic -> write full logs to ../logs/diag_decode_output.txt\n",
    "import torch, torch.nn.functional as F, json\n",
    "from pathlib import Path\n",
    "from src.evaluate import load_tokenizers_and_config\n",
    "from src.utils import get_device\n",
    "\n",
    "tokenizer_info, sp_vi, sp_en = load_tokenizers_and_config()\n",
    "device = get_device()\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def piece_for_id(sp, i):\n",
    "    try:\n",
    "        return sp.IdToPiece(i)\n",
    "    except Exception:\n",
    "        try:\n",
    "            return sp.id_to_piece(i)\n",
    "        except Exception:\n",
    "            try:\n",
    "                return sp.decode_ids([i])\n",
    "            except Exception:\n",
    "                return f\"<id:{i}>\"\n",
    "\n",
    "logs = []\n",
    "def log(s):\n",
    "    logs.append(str(s))\n",
    "\n",
    "def inspect_example_to_logs(src_text, max_len=60):\n",
    "    src_ids = sp_vi.encode_as_ids(src_text)\n",
    "    src_ids = [tokenizer_info['bos_id']] + src_ids + [tokenizer_info['eos_id']]\n",
    "    src = torch.tensor(src_ids, dtype=torch.long, device=device).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        encoder_output, src_mask = model.encode(src)\n",
    "    tgt_ids = [tokenizer_info['bos_id']]\n",
    "    log(f\"SRC: {src_text}\")\n",
    "    log(f\"SRC ids: {src_ids}\")\n",
    "    for step in range(max_len):\n",
    "        tgt = torch.tensor(tgt_ids, dtype=torch.long, device=device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            out = model.decode(tgt, encoder_output, src_mask)\n",
    "        logits = out[0, -1, :]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        topk = torch.topk(probs, k=10)\n",
    "        next_id = int(torch.argmax(logits).item())\n",
    "        log(f\"Step {step+1}: next_id={next_id} piece={piece_for_id(sp_en, next_id)} prob={probs[next_id].item():.6f}\")\n",
    "        log('  Top10: ' + json.dumps([(int(idx), piece_for_id(sp_en,int(idx)), float(p)) for idx,p in zip(topk.indices.tolist(), topk.values.tolist())]))\n",
    "        ent = -(probs * torch.log(probs + 1e-12)).sum().item()\n",
    "        log(f\"  Entropy: {ent:.6f}\")\n",
    "        tgt_ids.append(next_id)\n",
    "        if next_id == tokenizer_info['eos_id']:\n",
    "            log('  EOS reached.')\n",
    "            break\n",
    "    log(f\"Decoded ids: {tgt_ids}\")\n",
    "    decoded = [i for i in tgt_ids[1:] if i != tokenizer_info['eos_id'] and i != tokenizer_info['pad_id']]\n",
    "    try:\n",
    "        log('Decoded text (robust): ' + sp_en.decode_ids(decoded))\n",
    "    except Exception:\n",
    "        log('Decoded text (fallback): ' + ''.join([piece_for_id(sp_en,i) for i in decoded]).replace('▁',' ').strip())\n",
    "    log('----')\n",
    "\n",
    "examples = [\n",
    "    \"Lý tưởng nhất là bạn bắt đầu ngày mới với 10-15 phút thiền , hoặc áp dụng các kỹ thuật thư giãn tinh thần với bài tập hít thở .\",\n",
    "    \"Chỉ cần vài phút ngồi thiền cũng đủ đảm bảo một khởi đầu tích cực trong ngày , giúp bạn đối phó tốt hơn với sự căng thẳng .\",\n",
    "    \"Giảm căng thẳng cũng đồng nghĩa giảm tình trạng viêm gây tắc các ống dẫn trong cơ thể .\"\n",
    "]\n",
    "for ex in examples:\n",
    "    inspect_example_to_logs(ex)\n",
    "\n",
    "out_path = Path('..') / 'logs' / 'diag_decode_output.txt'\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(out_path, 'w', encoding='utf-8') as f:\n",
    "    for line in logs:\n",
    "        f.write(line + '\\n')\n",
    "print(f'Wrote diagnostic logs to: {out_path.resolve()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0c28b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src shape: torch.Size([8, 63])\n",
      "tgt shape: torch.Size([8, 58])\n",
      "src[0]: [2, 116, 388, 497, 1111, 6281, 36, 385, 495, 680, 510, 82, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "tgt[0]: [2, 95, 12, 13842, 5530, 376, 324, 93, 1294, 4715, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "src_text: Và hệ thống chữ Indus có tính chất đặc biệt này\n",
      "tgt_text: And the Indus script now has this particular property .\n"
     ]
    }
   ],
   "source": [
    "# Lấy 1 batch từ train_loader để kiểm tra\n",
    "batch = next(iter(train_loader))\n",
    "src = batch['src']\n",
    "tgt = batch['tgt']\n",
    "print('src shape:', src.shape)\n",
    "print('tgt shape:', tgt.shape)\n",
    "print('src[0]:', src[0].tolist())\n",
    "print('tgt[0]:', tgt[0].tolist())\n",
    "print('src_text:', train_dataset.sp_src.decode_ids([i for i in src[0].tolist() if i != pad_id]))\n",
    "print('tgt_text:', train_dataset.sp_tgt.decode_ids([i for i in tgt[0].tolist() if i != pad_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83769be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits shape: torch.Size([8, 57, 32000])\n",
      "logits[0,0,:10]: [-0.5386893  -0.7801632  -0.32187152  2.3923707  -0.95442057 -0.60122186\n",
      " -0.16280548  1.167485   -0.1504313   2.2502346 ]\n",
      "logits min: -1.6018061637878418 | max: 2.59318208694458 | mean: -0.6912190318107605\n"
     ]
    }
   ],
   "source": [
    "# Đưa batch lên device, tạo input cho model\n",
    "src = src.to(device)\n",
    "tgt = tgt.to(device)\n",
    "tgt_input = tgt[:, :-1]\n",
    "tgt_output = tgt[:, 1:]\n",
    "with torch.no_grad():\n",
    "    logits = model(src, tgt_input)  # [B, T, vocab_size]\n",
    "print('logits shape:', logits.shape)\n",
    "print('logits[0,0,:10]:', logits[0,0,:10].cpu().numpy())\n",
    "print('logits min:', logits.min().item(), '| max:', logits.max().item(), '| mean:', logits.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa32801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LabelSmoothingLoss: 3.7305 | CrossEntropyLoss: 9.1884\n"
     ]
    }
   ],
   "source": [
    "# Tính loss trên batch này, so sánh với cross-entropy không smoothing\n",
    "from torch.nn import CrossEntropyLoss\n",
    "logits_flat = logits.contiguous().view(-1, logits.size(-1))\n",
    "tgt_output_flat = tgt_output.contiguous().view(-1)\n",
    "loss_label_smooth = criterion(logits_flat, tgt_output_flat).item()\n",
    "ce_loss = CrossEntropyLoss(ignore_index=pad_id)\n",
    "loss_ce = ce_loss(logits_flat, tgt_output_flat).item()\n",
    "print(f\"LabelSmoothingLoss: {loss_label_smooth:.4f} | CrossEntropyLoss: {loss_ce:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a54d41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs[0,0,:10]: [3.4578192e-05 2.7160109e-05 4.2950192e-05 6.4825447e-04 2.2816685e-05\n",
      " 3.2482152e-05 5.0355458e-05 1.9045149e-04 5.0982435e-05 5.6236284e-04]\n",
      "probs[0,0] sum: 1.0\n",
      "probs max: 0.0007970688166096807 | min: 1.2010872524115257e-05 | mean: 3.125000148429535e-05\n"
     ]
    }
   ],
   "source": [
    "# Kiểm tra phân phối xác suất sau softmax (có bị collapse không)\n",
    "import torch.nn.functional as F\n",
    "probs = F.softmax(logits, dim=-1)\n",
    "print('probs[0,0,:10]:', probs[0,0,:10].cpu().numpy())\n",
    "print('probs[0,0] sum:', probs[0,0].sum().item())\n",
    "print('probs max:', probs.max().item(), '| min:', probs.min().item(), '| mean:', probs.mean().item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
