# =============================================================================
# Transformer English-Vietnamese Translation Model Configuration
# =============================================================================

# -----------------------------------------------------------------------------
# Data Configuration
# -----------------------------------------------------------------------------
data:
  # Data source: "local", "huggingface", or "processed"
  # - "local": Load from raw text files, tokenize on-the-fly
  # - "processed": Load pre-tokenized data from .pt files (fastest)
  # Run: python scripts/preprocess_data.py to create processed data
  source: "local"
  
  # Hugging Face settings (for download script)
  dataset_name: "vinai/PhoMT"
  # NOTE: hf_token should be set in .env file, not here!
  
  # Local file paths (PhoMT raw data)
  train_src: "data/PhoMT/detokenization/train/train.en"
  train_tgt: "data/PhoMT/detokenization/train/train.vi"
  val_src: "data/PhoMT/detokenization/dev/dev.en"
  val_tgt: "data/PhoMT/detokenization/dev/dev.vi"
  test_src: "data/PhoMT/detokenization/test/test.en"
  test_tgt: "data/PhoMT/detokenization/test/test.vi"
  
  # Processed data paths (after running preprocess_data.py)
  processed_train: "data/processed/train.pt"
  processed_val: "data/processed/val.pt"
  processed_test: "data/processed/test.pt"
  
  # Evaluation limits (to save time/cost)
  eval_max_samples_bleu: 1000  # For BLEU score
  eval_max_samples_gemini: 200 # For Gemini score
  
  # Preprocessing
  max_seq_len: 128
  min_seq_len: 1
  max_samples: null  # null = all samples, set number to limit
  
  # Vocabulary (increased for better coverage)
  src_vocab_size: 16000
  tgt_vocab_size: 16000

# -----------------------------------------------------------------------------
# Model Configuration (Optimized for speed + quality)
# -----------------------------------------------------------------------------
model:
  d_model: 256           # Smaller for faster training
  num_heads: 8           # Number of attention heads (256/8=32 per head)
  num_encoder_layers: 5  # Number of encoder layers
  num_decoder_layers: 6  # More decoder layers for Vietnamese (target language)
  d_ff: 1024             # Feed-forward dimension (4x d_model)
  dropout: 0.15          # Slightly higher for better regularization
  max_seq_len: 128       # Maximum sequence length

# -----------------------------------------------------------------------------
# Training Configuration (Optimized for 3M samples + limited GPU)
# -----------------------------------------------------------------------------
training:
  batch_size: 64         # Good for most GPUs
  gradient_accumulation_steps: 1  # Effective batch = 64
  epochs: 15             # More epochs for smaller model
  learning_rate: 0.0008  # Slightly lower for stability
  warmup_steps: 1500     # Shorter warmup (faster start)
  label_smoothing: 0.1   # Standard value
  gradient_clip: 1.0
  
  # Optimizer (AdamW with weight decay)
  optimizer: "adamw"
  betas: [0.9, 0.98]
  eps: 1.0e-9
  weight_decay: 0.05     # Higher for better regularization
  
  # Scheduler: Cosine Annealing with Linear Warmup
  min_lr: 1.0e-6             # Minimum LR at end of training
  
  # Checkpointing
  save_every: 1           # Save every N epochs
  eval_every: 1           # Evaluate every N epochs
  
  # Early stopping
  early_stopping_patience: 5

# -----------------------------------------------------------------------------
# Paths (will be overridden based on environment)
# -----------------------------------------------------------------------------
paths:
  data_dir: "data/"
  checkpoint_dir: "checkpoints/"
  log_dir: "logs/"

# -----------------------------------------------------------------------------
# Output Files - Tên các file được sinh ra khi training
# -----------------------------------------------------------------------------
output_files:
  # Tokenizer models (SentencePiece)
  tokenizer_src: "tokenizer_src.model"      # Source language tokenizer
  tokenizer_tgt: "tokenizer_tgt.model"      # Target language tokenizer
  vocab_src: "tokenizer_src.vocab"          # Source vocabulary file
  vocab_tgt: "tokenizer_tgt.vocab"          # Target vocabulary file
  
  # Model checkpoints
  best_model: "best_model.pt"               # Best model checkpoint
  last_model: "last_model.pt"               # Last epoch checkpoint
  checkpoint_pattern: "checkpoint_epoch_{epoch}.pt"  # Per-epoch checkpoints
  
  # Processed data (optional cache)
  processed_train: "processed_train.pt"     # Preprocessed training data
  processed_val: "processed_val.pt"         # Preprocessed validation data

# -----------------------------------------------------------------------------
# Logging
# -----------------------------------------------------------------------------
logging:
  use_tensorboard: true
  log_every: 100          # Log every N steps

# -----------------------------------------------------------------------------
# Hardware
# -----------------------------------------------------------------------------
hardware:
  device: "auto"          # auto, cuda, cpu
  num_workers: 4          # 4 for production
  pin_memory: true
